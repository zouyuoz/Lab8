{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceedafe-2782-41a1-8119-50ee3d6c21fd",
   "metadata": {},
   "source": [
    "# 2025 DL Lab8: RL Assignment_Super Mario World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa555a-e61c-4fb3-b5d0-289b66570139",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm XXX, XXXXXXXXXX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b0974-9605-488a-9fd5-00816e7832cc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This project implements a **Deep Reinforcement Learning** pipeline to train an autonomous agent for Super Mario World. Leveraging the **Proximal Policy Optimization (PPO)** algorithm, the system interacts with the **stable-retro** environment to master the YoshiIsland1 level. Key components include a custom Vision Backbone for extracting features from raw pixel data and a suite of Environment Wrappers that handle frame preprocessing, action discretization, and reward shaping to facilitate efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02696447",
   "metadata": {},
   "source": [
    "Reward function implement  \n",
    "should do something in the beginning (monster attack)  \n",
    "Custom PPO implement  \n",
    "pre train weight 差不多，主要是 reward function  \n",
    "model weight capacity 1GB  \n",
    "class name 不要動 (可以新增，但是原本有的不要動)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a0ab9-f86d-4038-833d-761ec81fc4f2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b10def-362c-4910-9ed0-f3d0904343ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import retro\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from eval import evaluate_policy, record_video\n",
    "from custom_policy import VisionBackbonePolicy, CustomPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10361fd-f291-4d93-b50d-cc749a3af588",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f6a25-738c-49dd-8e66-ae164b74a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Settings\n",
    "GAME = \"SuperMarioWorld-Snes\"\n",
    "STATE = \"YoshiIsland1\"\n",
    "\n",
    "# Training Settings\n",
    "# TOTAL_STEPS = 0xA00000 # 10,485,760\n",
    "TOTAL_STEPS = 0x1400000 # 20,971,520\n",
    "TRAIN_CHUNK = 0x0040000 #    262,144\n",
    "N_ENVS = 16\n",
    "\n",
    "# Evaluation & Recording Settings\n",
    "EVAL_EPISODES = 3\n",
    "EVAL_MAX_STEPS = 18000\n",
    "RECORD_STEPS = 1200\n",
    "\n",
    "# Directories\n",
    "LOG_DIR = \"./runs_smw\"\n",
    "VIDEO_DIR       = os.path.join(LOG_DIR, \"videos\")\n",
    "CKPT_DIR        = os.path.join(LOG_DIR, \"checkpoints\")\n",
    "TENSORBOARD_LOG = os.path.join(LOG_DIR, \"tb\")\n",
    "\n",
    "os.makedirs(LOG_DIR,   exist_ok=True)\n",
    "os.makedirs(CKPT_DIR,  exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34b783-0273-4835-ad2e-f9186064f76f",
   "metadata": {},
   "source": [
    "## Environment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c34213d-2c7c-42b8-922d-bafa285d1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_base_env\n",
    "def _make_env_thunk(game: str, state: str):\n",
    "    \"\"\"Return a function that creates an environment (for multiprocessing).\"\"\"\n",
    "    def _thunk():\n",
    "        return make_base_env(game, state)\n",
    "    return _thunk\n",
    "\n",
    "def make_vec_env(game: str, state: str, n_envs: int, use_subproc: bool = True):\n",
    "    \"\"\"Create a vectorized environment (multiple envs running in parallel).\"\"\"\n",
    "    env_fns = [_make_env_thunk(game, state) for _ in range(n_envs)]\n",
    "    \n",
    "    if use_subproc and n_envs > 1:\n",
    "        vec_env = SubprocVecEnv(env_fns)\n",
    "    else:\n",
    "        vec_env = DummyVecEnv(env_fns)\n",
    "\n",
    "    return vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dff476-ea2e-4262-8780-afb32ef1b233",
   "metadata": {},
   "source": [
    "## Initialize Env & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7c5fe-6421-4dbc-9bd8-822d61769c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created: SuperMarioWorld-Snes - YoshiIsland1 with 16 parallel envs.\n",
      "[Fail] Can't load None. Will use new model\n",
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Training Environment\n",
    "train_env = make_vec_env(GAME, STATE, n_envs=N_ENVS)\n",
    "# train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)\n",
    "print(f\"Environment created: {GAME} - {STATE} with {N_ENVS} parallel envs.\")\n",
    "\n",
    "checkpoint_path = \"None\" # 6225920 (19) 有破壞\n",
    "checkpoint_path = \"runs_smw/checkpoints/Dec22A_39.zip\"\n",
    "\n",
    "best_mean = -1e18\n",
    "trained = 0\n",
    "round_idx = 0\n",
    "\n",
    "# 2. Initialize Model\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # 讀取現有模型\n",
    "    model = CustomPPO.load(\n",
    "        checkpoint_path, \n",
    "        env=train_env,\n",
    "        device=\"cuda:0\" # 確保使用 GPU\n",
    "    )\n",
    "    trained = model.num_timesteps\n",
    "    round_idx = int(trained / TRAIN_CHUNK)\n",
    "    print(f\"[Sucess] Loaded model from {checkpoint_path}\")\n",
    "    print(f\"trained: {trained}, round_index: {round_idx}\")\n",
    "else:\n",
    "    print(f\"[Fail] Can't load {checkpoint_path}. Will use new model\")\n",
    "    model = CustomPPO(\n",
    "        VisionBackbonePolicy,\n",
    "        train_env,\n",
    "        policy_kwargs   = dict(normalize_images=False),\n",
    "        n_epochs        = 4,\n",
    "        n_steps         = 512,\n",
    "        batch_size      = 512,\n",
    "        learning_rate   = 1e-4,\n",
    "        verbose         = 1,\n",
    "        gamma           = 0.96875,\n",
    "        kl_coef         = 1,\n",
    "        clip_range      = 0.125,\n",
    "        ent_coef        = 0.0375,\n",
    "        tensorboard_log = TENSORBOARD_LOG,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb3d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb7a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# from custom_policy import CustomPPO\n",
    "# from eval import record_video  # 確保 eval.py 在同一目錄下\n",
    "\n",
    "# # ================= 設定區 =================\n",
    "# # 遊戲設定 (請確保跟訓練時一致)\n",
    "# # target_numbers = [3932160, 6225920, 12451840] \n",
    "\n",
    "# # 方法 B: 自動搜尋資料夾下所有 PIPE_{number}.zip (如果你想全部測的話，把下面解註解)\n",
    "# files = glob.glob(os.path.join(CKPT_DIR, \"SF84G_*.zip\"))\n",
    "# target_numbers = list(range(38, 40))\n",
    "\n",
    "# # ================= 執行迴圈 =================\n",
    "# print(f\"準備測試以下 Checkpoints: {target_numbers}\")\n",
    "\n",
    "# for num in target_numbers:\n",
    "#     model_path = os.path.join(CKPT_DIR, f\"PIPE_{num}.zip\")\n",
    "    \n",
    "#     # 檢查檔案是否存在\n",
    "#     if not os.path.exists(model_path):\n",
    "#         print(f\"⚠️ 找不到檔案: {model_path}，跳過。\")\n",
    "#         continue\n",
    "    \n",
    "#     print(f\"\\n[{num}] 正在載入模型: {model_path} ...\")\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 載入模型 (不需要 env 參數也能載入權重)\n",
    "#         # 如果你有改過 CustomPPO 的參數，load 會自動讀取 zip 裡的設定\n",
    "#         model = CustomPPO.load(model_path, device=\"auto\") # device=\"auto\" 會自動用 GPU\n",
    "        \n",
    "#         # 2. 錄製影片\n",
    "#         prefix_name = f\"test_{num}\"\n",
    "#         print(f\"[{num}] 正在錄影 (長度 {RECORD_STEPS} steps)...\")\n",
    "        \n",
    "#         record_video(\n",
    "#             model=model,\n",
    "#             game=GAME,\n",
    "#             state=STATE,\n",
    "#             out_dir=VIDEO_DIR,\n",
    "#             video_len=RECORD_STEPS,\n",
    "#             prefix=prefix_name\n",
    "#         )\n",
    "#         print(f\"✅ 完成！影片已儲存為 {prefix_name}.mp4\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ 發生錯誤 (Model: {num}): {e}\")\n",
    "\n",
    "# print(\"\\n所有測試結束。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594e443-843f-42c1-9fc6-3fbc82962021",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af4932-c531-4113-a33a-defc6fb5858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 | Learn 262144 steps (Total trained: 0) ===\n",
      "Logging to ./runs_smw/tb/SLR_0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1408 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1132          |\n",
      "|    iterations         | 2             |\n",
      "|    time_elapsed       | 14            |\n",
      "|    total_timesteps    | 16384         |\n",
      "| train/                |               |\n",
      "|    approx_kl          | 0.00075144175 |\n",
      "|    entropy_loss       | -2.3          |\n",
      "|    explained_variance | -0.028        |\n",
      "|    learning_rate      | 0.0001        |\n",
      "|    loss               | -0.077        |\n",
      "|    mean_step_reward   | 0.010707685   |\n",
      "|    n_updates          | 4             |\n",
      "|    policyGradLoss     | -0.00052      |\n",
      "|    value_loss         | 0.0294        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1059        |\n",
      "|    iterations         | 3           |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 24576       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.001850405 |\n",
      "|    entropy_loss       | -2.3        |\n",
      "|    explained_variance | -0.308      |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0853     |\n",
      "|    mean_step_reward   | 0.011003539 |\n",
      "|    n_updates          | 8           |\n",
      "|    policyGradLoss     | -0.00101    |\n",
      "|    value_loss         | 0.00885     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1016         |\n",
      "|    iterations         | 4            |\n",
      "|    time_elapsed       | 32           |\n",
      "|    total_timesteps    | 32768        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023637114 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.202        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0715      |\n",
      "|    mean_step_reward   | 0.010230764  |\n",
      "|    n_updates          | 12           |\n",
      "|    policyGradLoss     | -0.00137     |\n",
      "|    value_loss         | 0.027        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 988          |\n",
      "|    iterations         | 5            |\n",
      "|    time_elapsed       | 41           |\n",
      "|    total_timesteps    | 40960        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021635308 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.377        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0817      |\n",
      "|    mean_step_reward   | 0.010496189  |\n",
      "|    n_updates          | 16           |\n",
      "|    policyGradLoss     | -0.00103     |\n",
      "|    value_loss         | 0.0211       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 981          |\n",
      "|    iterations         | 6            |\n",
      "|    time_elapsed       | 50           |\n",
      "|    total_timesteps    | 49152        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0022081374 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.162        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0608      |\n",
      "|    mean_step_reward   | 0.011018835  |\n",
      "|    n_updates          | 20           |\n",
      "|    policyGradLoss     | -0.00105     |\n",
      "|    value_loss         | 0.0482       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 979          |\n",
      "|    iterations         | 7            |\n",
      "|    time_elapsed       | 58           |\n",
      "|    total_timesteps    | 57344        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0018326206 |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | 0.437        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0711      |\n",
      "|    mean_step_reward   | 0.009761704  |\n",
      "|    n_updates          | 24           |\n",
      "|    policyGradLoss     | -0.000946    |\n",
      "|    value_loss         | 0.0341       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 978          |\n",
      "|    iterations         | 8            |\n",
      "|    time_elapsed       | 66           |\n",
      "|    total_timesteps    | 65536        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0011834848 |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | 0.593        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0752      |\n",
      "|    mean_step_reward   | 0.009815867  |\n",
      "|    n_updates          | 28           |\n",
      "|    policyGradLoss     | -0.000388    |\n",
      "|    value_loss         | 0.0278       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 978          |\n",
      "|    iterations         | 9            |\n",
      "|    time_elapsed       | 75           |\n",
      "|    total_timesteps    | 73728        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021785884 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.595        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0694      |\n",
      "|    mean_step_reward   | 0.009929255  |\n",
      "|    n_updates          | 32           |\n",
      "|    policyGradLoss     | -0.000685    |\n",
      "|    value_loss         | 0.0329       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 976          |\n",
      "|    iterations         | 10           |\n",
      "|    time_elapsed       | 83           |\n",
      "|    total_timesteps    | 81920        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0030344417 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.536        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0697      |\n",
      "|    mean_step_reward   | 0.0096684825 |\n",
      "|    n_updates          | 36           |\n",
      "|    policyGradLoss     | -0.00192     |\n",
      "|    value_loss         | 0.028        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 968          |\n",
      "|    iterations         | 11           |\n",
      "|    time_elapsed       | 93           |\n",
      "|    total_timesteps    | 90112        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0028127655 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.648        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.079       |\n",
      "|    mean_step_reward   | 0.01027053   |\n",
      "|    n_updates          | 40           |\n",
      "|    policyGradLoss     | -0.00106     |\n",
      "|    value_loss         | 0.0184       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 956          |\n",
      "|    iterations         | 12           |\n",
      "|    time_elapsed       | 102          |\n",
      "|    total_timesteps    | 98304        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0014983403 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.488        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0762      |\n",
      "|    mean_step_reward   | 0.009829838  |\n",
      "|    n_updates          | 44           |\n",
      "|    policyGradLoss     | -0.000739    |\n",
      "|    value_loss         | 0.0207       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 945         |\n",
      "|    iterations         | 13          |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 106496      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.003034946 |\n",
      "|    entropy_loss       | -2.3        |\n",
      "|    explained_variance | 0.479       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0783     |\n",
      "|    mean_step_reward   | 0.010173237 |\n",
      "|    n_updates          | 48          |\n",
      "|    policyGradLoss     | -0.00154    |\n",
      "|    value_loss         | 0.00974     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 936          |\n",
      "|    iterations         | 14           |\n",
      "|    time_elapsed       | 122          |\n",
      "|    total_timesteps    | 114688       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0027361622 |\n",
      "|    entropy_loss       | -2.3         |\n",
      "|    explained_variance | 0.555        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0924      |\n",
      "|    mean_step_reward   | 0.010272611  |\n",
      "|    n_updates          | 52           |\n",
      "|    policyGradLoss     | -0.00273     |\n",
      "|    value_loss         | 0.00179      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 928          |\n",
      "|    iterations         | 15           |\n",
      "|    time_elapsed       | 132          |\n",
      "|    total_timesteps    | 122880       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0031948015 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.179        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0833      |\n",
      "|    mean_step_reward   | 0.0113708805 |\n",
      "|    n_updates          | 56           |\n",
      "|    policyGradLoss     | -0.00316     |\n",
      "|    value_loss         | 0.0216       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 921          |\n",
      "|    iterations         | 16           |\n",
      "|    time_elapsed       | 142          |\n",
      "|    total_timesteps    | 131072       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019612245 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.485        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0778      |\n",
      "|    mean_step_reward   | 0.009768764  |\n",
      "|    n_updates          | 60           |\n",
      "|    policyGradLoss     | -0.00177     |\n",
      "|    value_loss         | 0.0212       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 915          |\n",
      "|    iterations         | 17           |\n",
      "|    time_elapsed       | 152          |\n",
      "|    total_timesteps    | 139264       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019786619 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.543        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0678      |\n",
      "|    mean_step_reward   | 0.009184746  |\n",
      "|    n_updates          | 64           |\n",
      "|    policyGradLoss     | -0.000693    |\n",
      "|    value_loss         | 0.0339       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 910          |\n",
      "|    iterations         | 18           |\n",
      "|    time_elapsed       | 161          |\n",
      "|    total_timesteps    | 147456       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024268404 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.627        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0753      |\n",
      "|    mean_step_reward   | 0.0100976005 |\n",
      "|    n_updates          | 68           |\n",
      "|    policyGradLoss     | -0.00156     |\n",
      "|    value_loss         | 0.0256       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 908          |\n",
      "|    iterations         | 19           |\n",
      "|    time_elapsed       | 171          |\n",
      "|    total_timesteps    | 155648       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026759543 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.614        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0828      |\n",
      "|    mean_step_reward   | 0.011350149  |\n",
      "|    n_updates          | 72           |\n",
      "|    policyGradLoss     | -0.00158     |\n",
      "|    value_loss         | 0.00781      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 906         |\n",
      "|    iterations         | 20          |\n",
      "|    time_elapsed       | 180         |\n",
      "|    total_timesteps    | 163840      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002608519 |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0.63        |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0877     |\n",
      "|    mean_step_reward   | 0.010178384 |\n",
      "|    n_updates          | 76          |\n",
      "|    policyGradLoss     | -0.00172    |\n",
      "|    value_loss         | 0.0112      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 904         |\n",
      "|    iterations         | 21          |\n",
      "|    time_elapsed       | 190         |\n",
      "|    total_timesteps    | 172032      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.00242227  |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0.542       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0794     |\n",
      "|    mean_step_reward   | 0.010570515 |\n",
      "|    n_updates          | 80          |\n",
      "|    policyGradLoss     | -0.000938   |\n",
      "|    value_loss         | 0.0059      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 907          |\n",
      "|    iterations         | 22           |\n",
      "|    time_elapsed       | 198          |\n",
      "|    total_timesteps    | 180224       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0012093075 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.314        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0769      |\n",
      "|    mean_step_reward   | 0.011245972  |\n",
      "|    n_updates          | 84           |\n",
      "|    policyGradLoss     | -0.000997    |\n",
      "|    value_loss         | 0.0284       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 910          |\n",
      "|    iterations         | 23           |\n",
      "|    time_elapsed       | 207          |\n",
      "|    total_timesteps    | 188416       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0010131586 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.732        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0813      |\n",
      "|    mean_step_reward   | 0.010761685  |\n",
      "|    n_updates          | 88           |\n",
      "|    policyGradLoss     | -0.000917    |\n",
      "|    value_loss         | 0.00515      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 910          |\n",
      "|    iterations         | 24           |\n",
      "|    time_elapsed       | 216          |\n",
      "|    total_timesteps    | 196608       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021927264 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.506        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0749      |\n",
      "|    mean_step_reward   | 0.0108017875 |\n",
      "|    n_updates          | 92           |\n",
      "|    policyGradLoss     | -0.00117     |\n",
      "|    value_loss         | 0.0253       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 909          |\n",
      "|    iterations         | 25           |\n",
      "|    time_elapsed       | 225          |\n",
      "|    total_timesteps    | 204800       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0016038951 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.487        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0839      |\n",
      "|    mean_step_reward   | 0.010179436  |\n",
      "|    n_updates          | 96           |\n",
      "|    policyGradLoss     | -0.00154     |\n",
      "|    value_loss         | 0.0137       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 909          |\n",
      "|    iterations         | 26           |\n",
      "|    time_elapsed       | 234          |\n",
      "|    total_timesteps    | 212992       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0013360153 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.465        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0867      |\n",
      "|    mean_step_reward   | 0.010558875  |\n",
      "|    n_updates          | 100          |\n",
      "|    policyGradLoss     | -0.000788    |\n",
      "|    value_loss         | 0.00536      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 909          |\n",
      "|    iterations         | 27           |\n",
      "|    time_elapsed       | 243          |\n",
      "|    total_timesteps    | 221184       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021502324 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 0.533        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0854      |\n",
      "|    mean_step_reward   | 0.010865551  |\n",
      "|    n_updates          | 104          |\n",
      "|    policyGradLoss     | -0.00198     |\n",
      "|    value_loss         | 0.00706      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 907          |\n",
      "|    iterations         | 28           |\n",
      "|    time_elapsed       | 252          |\n",
      "|    total_timesteps    | 229376       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0030077405 |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | -2.44        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0915      |\n",
      "|    mean_step_reward   | 0.011129471  |\n",
      "|    n_updates          | 108          |\n",
      "|    policyGradLoss     | -0.00364     |\n",
      "|    value_loss         | 0.000748     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 906          |\n",
      "|    iterations         | 29           |\n",
      "|    time_elapsed       | 262          |\n",
      "|    total_timesteps    | 237568       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019023756 |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | -0.119       |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0863      |\n",
      "|    mean_step_reward   | 0.011294453  |\n",
      "|    n_updates          | 112          |\n",
      "|    policyGradLoss     | -0.0021      |\n",
      "|    value_loss         | 0.00232      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 904         |\n",
      "|    iterations         | 30          |\n",
      "|    time_elapsed       | 271         |\n",
      "|    total_timesteps    | 245760      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.001889075 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.015       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0689     |\n",
      "|    mean_step_reward   | 0.011556962 |\n",
      "|    n_updates          | 116         |\n",
      "|    policyGradLoss     | -0.00229    |\n",
      "|    value_loss         | 0.0251      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 903         |\n",
      "|    iterations         | 31          |\n",
      "|    time_elapsed       | 281         |\n",
      "|    total_timesteps    | 253952      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002547178 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.305       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0787     |\n",
      "|    mean_step_reward   | 0.01112566  |\n",
      "|    n_updates          | 120         |\n",
      "|    policyGradLoss     | -0.00232    |\n",
      "|    value_loss         | 0.0329      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 900         |\n",
      "|    iterations         | 32          |\n",
      "|    time_elapsed       | 290         |\n",
      "|    total_timesteps    | 262144      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.003784469 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.31        |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0841     |\n",
      "|    mean_step_reward   | 0.012078809 |\n",
      "|    n_updates          | 124         |\n",
      "|    policyGradLoss     | -0.0046     |\n",
      "|    value_loss         | 0.0493      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SLR_0.zip\n",
      "[EVAL] Mean Return: -0.471, Best Return: -0.471\n",
      "Saved video to ./runs_smw/videos/SLR/SLR_0_-0.47.mp4\n",
      "\n",
      "=== Round 2 | Learn 262144 steps (Total trained: 262144) ===\n",
      "Logging to ./runs_smw/tb/SLR_0\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1331   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 6      |\n",
      "|    total_timesteps | 270336 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1044        |\n",
      "|    iterations         | 2           |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 278528      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.001845774 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.686       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0821     |\n",
      "|    mean_step_reward   | 0.009730989 |\n",
      "|    n_updates          | 132         |\n",
      "|    policyGradLoss     | -0.00157    |\n",
      "|    value_loss         | 0.0182      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 997          |\n",
      "|    iterations         | 3            |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 286720       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0031271938 |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | 0.477        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0749      |\n",
      "|    mean_step_reward   | 0.01098455   |\n",
      "|    n_updates          | 136          |\n",
      "|    policyGradLoss     | -0.00293     |\n",
      "|    value_loss         | 0.0284       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 990          |\n",
      "|    iterations         | 4            |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 294912       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023061987 |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | 0.636        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0903      |\n",
      "|    mean_step_reward   | 0.011528201  |\n",
      "|    n_updates          | 140          |\n",
      "|    policyGradLoss     | -0.00404     |\n",
      "|    value_loss         | 0.00271      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 985          |\n",
      "|    iterations         | 5            |\n",
      "|    time_elapsed       | 41           |\n",
      "|    total_timesteps    | 303104       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019424167 |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.685        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0903      |\n",
      "|    mean_step_reward   | 0.011444781  |\n",
      "|    n_updates          | 144          |\n",
      "|    policyGradLoss     | -0.00316     |\n",
      "|    value_loss         | 0.00277      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 979          |\n",
      "|    iterations         | 6            |\n",
      "|    time_elapsed       | 50           |\n",
      "|    total_timesteps    | 311296       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020668048 |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.637        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0884      |\n",
      "|    mean_step_reward   | 0.011650363  |\n",
      "|    n_updates          | 148          |\n",
      "|    policyGradLoss     | -0.00275     |\n",
      "|    value_loss         | 0.00455      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 975          |\n",
      "|    iterations         | 7            |\n",
      "|    time_elapsed       | 58           |\n",
      "|    total_timesteps    | 319488       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024540424 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.4          |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0906      |\n",
      "|    mean_step_reward   | 0.0113224685 |\n",
      "|    n_updates          | 152          |\n",
      "|    policyGradLoss     | -0.00284     |\n",
      "|    value_loss         | 0.00194      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 963         |\n",
      "|    iterations         | 8           |\n",
      "|    time_elapsed       | 68          |\n",
      "|    total_timesteps    | 327680      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002121982 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.443       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.084      |\n",
      "|    mean_step_reward   | 0.011491437 |\n",
      "|    n_updates          | 156         |\n",
      "|    policyGradLoss     | -0.00216    |\n",
      "|    value_loss         | 0.00541     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 949          |\n",
      "|    iterations         | 9            |\n",
      "|    time_elapsed       | 77           |\n",
      "|    total_timesteps    | 335872       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0014473908 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.419        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0869      |\n",
      "|    mean_step_reward   | 0.011424445  |\n",
      "|    n_updates          | 160          |\n",
      "|    policyGradLoss     | -0.00148     |\n",
      "|    value_loss         | 0.00682      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 933          |\n",
      "|    iterations         | 10           |\n",
      "|    time_elapsed       | 87           |\n",
      "|    total_timesteps    | 344064       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020336695 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.601        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0897      |\n",
      "|    mean_step_reward   | 0.011503947  |\n",
      "|    n_updates          | 164          |\n",
      "|    policyGradLoss     | -0.00289     |\n",
      "|    value_loss         | 0.00227      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 921          |\n",
      "|    iterations         | 11           |\n",
      "|    time_elapsed       | 97           |\n",
      "|    total_timesteps    | 352256       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0018320492 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.423        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0854      |\n",
      "|    mean_step_reward   | 0.011210381  |\n",
      "|    n_updates          | 168          |\n",
      "|    policyGradLoss     | -0.00224     |\n",
      "|    value_loss         | 0.00286      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 912          |\n",
      "|    iterations         | 12           |\n",
      "|    time_elapsed       | 107          |\n",
      "|    total_timesteps    | 360448       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020716698 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.342        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0775      |\n",
      "|    mean_step_reward   | 0.011398867  |\n",
      "|    n_updates          | 172          |\n",
      "|    policyGradLoss     | -0.00123     |\n",
      "|    value_loss         | 0.00653      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 905          |\n",
      "|    iterations         | 13           |\n",
      "|    time_elapsed       | 117          |\n",
      "|    total_timesteps    | 368640       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023178465 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.655        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0907      |\n",
      "|    mean_step_reward   | 0.011551317  |\n",
      "|    n_updates          | 176          |\n",
      "|    policyGradLoss     | -0.00331     |\n",
      "|    value_loss         | 0.00238      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 899          |\n",
      "|    iterations         | 14           |\n",
      "|    time_elapsed       | 127          |\n",
      "|    total_timesteps    | 376832       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019198832 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.2          |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0664      |\n",
      "|    mean_step_reward   | 0.011879405  |\n",
      "|    n_updates          | 180          |\n",
      "|    policyGradLoss     | -0.00301     |\n",
      "|    value_loss         | 0.0243       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 894          |\n",
      "|    iterations         | 15           |\n",
      "|    time_elapsed       | 137          |\n",
      "|    total_timesteps    | 385024       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020800824 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.241        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.086       |\n",
      "|    mean_step_reward   | 0.012129022  |\n",
      "|    n_updates          | 184          |\n",
      "|    policyGradLoss     | -0.00271     |\n",
      "|    value_loss         | 0.0157       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 893          |\n",
      "|    iterations         | 16           |\n",
      "|    time_elapsed       | 146          |\n",
      "|    total_timesteps    | 393216       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0015382315 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.673        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0915      |\n",
      "|    mean_step_reward   | 0.0114623215 |\n",
      "|    n_updates          | 188          |\n",
      "|    policyGradLoss     | -0.00214     |\n",
      "|    value_loss         | 0.00303      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 891         |\n",
      "|    iterations         | 17          |\n",
      "|    time_elapsed       | 156         |\n",
      "|    total_timesteps    | 401408      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002028056 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.106       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0786     |\n",
      "|    mean_step_reward   | 0.012748852 |\n",
      "|    n_updates          | 192         |\n",
      "|    policyGradLoss     | -0.00306    |\n",
      "|    value_loss         | 0.0493      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 893          |\n",
      "|    iterations         | 18           |\n",
      "|    time_elapsed       | 165          |\n",
      "|    total_timesteps    | 409600       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023405796 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.127        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0329      |\n",
      "|    mean_step_reward   | 0.01346424   |\n",
      "|    n_updates          | 196          |\n",
      "|    policyGradLoss     | -0.00224     |\n",
      "|    value_loss         | 0.0589       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 897         |\n",
      "|    iterations         | 19          |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 417792      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.001577245 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.147       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0831     |\n",
      "|    mean_step_reward   | 0.01223251  |\n",
      "|    n_updates          | 200         |\n",
      "|    policyGradLoss     | -0.00269    |\n",
      "|    value_loss         | 0.0215      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 900          |\n",
      "|    iterations         | 20           |\n",
      "|    time_elapsed       | 182          |\n",
      "|    total_timesteps    | 425984       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020868666 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.601        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0843      |\n",
      "|    mean_step_reward   | 0.011001441  |\n",
      "|    n_updates          | 204          |\n",
      "|    policyGradLoss     | -0.00312     |\n",
      "|    value_loss         | 0.0101       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 902          |\n",
      "|    iterations         | 21           |\n",
      "|    time_elapsed       | 190          |\n",
      "|    total_timesteps    | 434176       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0018066997 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.611        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0857      |\n",
      "|    mean_step_reward   | 0.011382111  |\n",
      "|    n_updates          | 208          |\n",
      "|    policyGradLoss     | -0.00253     |\n",
      "|    value_loss         | 0.00579      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 905          |\n",
      "|    iterations         | 22           |\n",
      "|    time_elapsed       | 199          |\n",
      "|    total_timesteps    | 442368       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019774893 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.737        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0878      |\n",
      "|    mean_step_reward   | 0.011002956  |\n",
      "|    n_updates          | 212          |\n",
      "|    policyGradLoss     | -0.00221     |\n",
      "|    value_loss         | 0.00486      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 908          |\n",
      "|    iterations         | 23           |\n",
      "|    time_elapsed       | 207          |\n",
      "|    total_timesteps    | 450560       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0017067186 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.264        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0801      |\n",
      "|    mean_step_reward   | 0.0123034865 |\n",
      "|    n_updates          | 216          |\n",
      "|    policyGradLoss     | -0.00284     |\n",
      "|    value_loss         | 0.042        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 907          |\n",
      "|    iterations         | 24           |\n",
      "|    time_elapsed       | 216          |\n",
      "|    total_timesteps    | 458752       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0012223313 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.229        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0854      |\n",
      "|    mean_step_reward   | 0.011534158  |\n",
      "|    n_updates          | 220          |\n",
      "|    policyGradLoss     | -0.00213     |\n",
      "|    value_loss         | 0.00538      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 905          |\n",
      "|    iterations         | 25           |\n",
      "|    time_elapsed       | 226          |\n",
      "|    total_timesteps    | 466944       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020407748 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.666        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0917      |\n",
      "|    mean_step_reward   | 0.011296548  |\n",
      "|    n_updates          | 224          |\n",
      "|    policyGradLoss     | -0.00225     |\n",
      "|    value_loss         | 0.0064       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 902          |\n",
      "|    iterations         | 26           |\n",
      "|    time_elapsed       | 236          |\n",
      "|    total_timesteps    | 475136       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019884957 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.11         |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0844      |\n",
      "|    mean_step_reward   | 0.011411034  |\n",
      "|    n_updates          | 228          |\n",
      "|    policyGradLoss     | -0.00239     |\n",
      "|    value_loss         | 0.00253      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 898          |\n",
      "|    iterations         | 27           |\n",
      "|    time_elapsed       | 246          |\n",
      "|    total_timesteps    | 483328       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023653773 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.674        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0811      |\n",
      "|    mean_step_reward   | 0.01163207   |\n",
      "|    n_updates          | 232          |\n",
      "|    policyGradLoss     | -0.00327     |\n",
      "|    value_loss         | 0.00238      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 896          |\n",
      "|    iterations         | 28           |\n",
      "|    time_elapsed       | 255          |\n",
      "|    total_timesteps    | 491520       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0031339317 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.187        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0501      |\n",
      "|    mean_step_reward   | 0.012230715  |\n",
      "|    n_updates          | 236          |\n",
      "|    policyGradLoss     | -0.00293     |\n",
      "|    value_loss         | 0.045        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 893          |\n",
      "|    iterations         | 29           |\n",
      "|    time_elapsed       | 265          |\n",
      "|    total_timesteps    | 499712       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0019287117 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.191        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0798      |\n",
      "|    mean_step_reward   | 0.011812422  |\n",
      "|    n_updates          | 240          |\n",
      "|    policyGradLoss     | -0.00308     |\n",
      "|    value_loss         | 0.026        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 891          |\n",
      "|    iterations         | 30           |\n",
      "|    time_elapsed       | 275          |\n",
      "|    total_timesteps    | 507904       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021239596 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.205        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0782      |\n",
      "|    mean_step_reward   | 0.012269696  |\n",
      "|    n_updates          | 244          |\n",
      "|    policyGradLoss     | -0.00339     |\n",
      "|    value_loss         | 0.0236       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 889          |\n",
      "|    iterations         | 31           |\n",
      "|    time_elapsed       | 285          |\n",
      "|    total_timesteps    | 516096       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023347558 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.419        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0857      |\n",
      "|    mean_step_reward   | 0.010889236  |\n",
      "|    n_updates          | 248          |\n",
      "|    policyGradLoss     | -0.00325     |\n",
      "|    value_loss         | 0.00641      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 888          |\n",
      "|    iterations         | 32           |\n",
      "|    time_elapsed       | 295          |\n",
      "|    total_timesteps    | 524288       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026302305 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.414        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0802      |\n",
      "|    mean_step_reward   | 0.011649266  |\n",
      "|    n_updates          | 252          |\n",
      "|    policyGradLoss     | -0.00268     |\n",
      "|    value_loss         | 0.00262      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SLR_1.zip\n",
      "[EVAL] Mean Return: -0.471, Best Return: -0.471\n",
      "Saved video to ./runs_smw/videos/SLR/SLR_1_-0.47.mp4\n",
      "\n",
      "=== Round 3 | Learn 262144 steps (Total trained: 524288) ===\n",
      "Logging to ./runs_smw/tb/SLR_0\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1461   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 5      |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1162        |\n",
      "|    iterations         | 2           |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 540672      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002261924 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.237       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0808     |\n",
      "|    mean_step_reward   | 0.011718493 |\n",
      "|    n_updates          | 260         |\n",
      "|    policyGradLoss     | -0.00296    |\n",
      "|    value_loss         | 0.0279      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1087         |\n",
      "|    iterations         | 3            |\n",
      "|    time_elapsed       | 22           |\n",
      "|    total_timesteps    | 548864       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0022482327 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.3          |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0751      |\n",
      "|    mean_step_reward   | 0.013053634  |\n",
      "|    n_updates          | 264          |\n",
      "|    policyGradLoss     | -0.00271     |\n",
      "|    value_loss         | 0.0545       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1055         |\n",
      "|    iterations         | 4            |\n",
      "|    time_elapsed       | 31           |\n",
      "|    total_timesteps    | 557056       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0025473302 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.43         |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.087       |\n",
      "|    mean_step_reward   | 0.011510591  |\n",
      "|    n_updates          | 268          |\n",
      "|    policyGradLoss     | -0.00334     |\n",
      "|    value_loss         | 0.00535      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1032        |\n",
      "|    iterations         | 5           |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 565248      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002344432 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.286       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.04       |\n",
      "|    mean_step_reward   | 0.013410977 |\n",
      "|    n_updates          | 272         |\n",
      "|    policyGradLoss     | -0.00297    |\n",
      "|    value_loss         | 0.0655      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1021         |\n",
      "|    iterations         | 6            |\n",
      "|    time_elapsed       | 48           |\n",
      "|    total_timesteps    | 573440       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026256188 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.261        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0853      |\n",
      "|    mean_step_reward   | 0.011715291  |\n",
      "|    n_updates          | 276          |\n",
      "|    policyGradLoss     | -0.0029      |\n",
      "|    value_loss         | 0.0266       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 994          |\n",
      "|    iterations         | 7            |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 581632       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0018982677 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.341        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0719      |\n",
      "|    mean_step_reward   | 0.013323383  |\n",
      "|    n_updates          | 280          |\n",
      "|    policyGradLoss     | -0.00331     |\n",
      "|    value_loss         | 0.0491       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 972          |\n",
      "|    iterations         | 8            |\n",
      "|    time_elapsed       | 67           |\n",
      "|    total_timesteps    | 589824       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.001975746  |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.447        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0677      |\n",
      "|    mean_step_reward   | 0.0132121015 |\n",
      "|    n_updates          | 284          |\n",
      "|    policyGradLoss     | -0.00339     |\n",
      "|    value_loss         | 0.0459       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 958         |\n",
      "|    iterations         | 9           |\n",
      "|    time_elapsed       | 76          |\n",
      "|    total_timesteps    | 598016      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002158761 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.38        |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.083      |\n",
      "|    mean_step_reward   | 0.012711983 |\n",
      "|    n_updates          | 288         |\n",
      "|    policyGradLoss     | -0.0031     |\n",
      "|    value_loss         | 0.036       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 946          |\n",
      "|    iterations         | 10           |\n",
      "|    time_elapsed       | 86           |\n",
      "|    total_timesteps    | 606208       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024119308 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.498        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0835      |\n",
      "|    mean_step_reward   | 0.011390537  |\n",
      "|    n_updates          | 292          |\n",
      "|    policyGradLoss     | -0.0034      |\n",
      "|    value_loss         | 0.00807      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 932          |\n",
      "|    iterations         | 11           |\n",
      "|    time_elapsed       | 96           |\n",
      "|    total_timesteps    | 614400       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023842202 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.355        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0899      |\n",
      "|    mean_step_reward   | 0.011313653  |\n",
      "|    n_updates          | 296          |\n",
      "|    policyGradLoss     | -0.00329     |\n",
      "|    value_loss         | 0.00646      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 922          |\n",
      "|    iterations         | 12           |\n",
      "|    time_elapsed       | 106          |\n",
      "|    total_timesteps    | 622592       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0028175237 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.238        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0798      |\n",
      "|    mean_step_reward   | 0.01260548   |\n",
      "|    n_updates          | 300          |\n",
      "|    policyGradLoss     | -0.00415     |\n",
      "|    value_loss         | 0.03         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 914          |\n",
      "|    iterations         | 13           |\n",
      "|    time_elapsed       | 116          |\n",
      "|    total_timesteps    | 630784       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0027893197 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.272        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0731      |\n",
      "|    mean_step_reward   | 0.014067584  |\n",
      "|    n_updates          | 304          |\n",
      "|    policyGradLoss     | -0.00364     |\n",
      "|    value_loss         | 0.0537       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 908         |\n",
      "|    iterations         | 14          |\n",
      "|    time_elapsed       | 126         |\n",
      "|    total_timesteps    | 638976      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002544905 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.473       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0846     |\n",
      "|    mean_step_reward   | 0.013972386 |\n",
      "|    n_updates          | 308         |\n",
      "|    policyGradLoss     | -0.00413    |\n",
      "|    value_loss         | 0.0407      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 903          |\n",
      "|    iterations         | 15           |\n",
      "|    time_elapsed       | 135          |\n",
      "|    total_timesteps    | 647168       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0036912446 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.0673       |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.086       |\n",
      "|    mean_step_reward   | 0.01208045   |\n",
      "|    n_updates          | 312          |\n",
      "|    policyGradLoss     | -0.00311     |\n",
      "|    value_loss         | 0.0189       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 903          |\n",
      "|    iterations         | 16           |\n",
      "|    time_elapsed       | 145          |\n",
      "|    total_timesteps    | 655360       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0032604465 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.311        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0912      |\n",
      "|    mean_step_reward   | 0.010935535  |\n",
      "|    n_updates          | 316          |\n",
      "|    policyGradLoss     | -0.00145     |\n",
      "|    value_loss         | 0.00956      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 905         |\n",
      "|    iterations         | 17          |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 663552      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002486446 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.283       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0802     |\n",
      "|    mean_step_reward   | 0.013430502 |\n",
      "|    n_updates          | 320         |\n",
      "|    policyGradLoss     | -0.00306    |\n",
      "|    value_loss         | 0.0258      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 907          |\n",
      "|    iterations         | 18           |\n",
      "|    time_elapsed       | 162          |\n",
      "|    total_timesteps    | 671744       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0028785756 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.522        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0771      |\n",
      "|    mean_step_reward   | 0.013183928  |\n",
      "|    n_updates          | 324          |\n",
      "|    policyGradLoss     | -0.00306     |\n",
      "|    value_loss         | 0.0465       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 911          |\n",
      "|    iterations         | 19           |\n",
      "|    time_elapsed       | 170          |\n",
      "|    total_timesteps    | 679936       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026640962 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.604        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0703      |\n",
      "|    mean_step_reward   | 0.013138598  |\n",
      "|    n_updates          | 328          |\n",
      "|    policyGradLoss     | -0.00402     |\n",
      "|    value_loss         | 0.0424       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 914         |\n",
      "|    iterations         | 20          |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 688128      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.00249903  |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.288       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0604     |\n",
      "|    mean_step_reward   | 0.012499993 |\n",
      "|    n_updates          | 332         |\n",
      "|    policyGradLoss     | -0.00256    |\n",
      "|    value_loss         | 0.0433      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 916          |\n",
      "|    iterations         | 21           |\n",
      "|    time_elapsed       | 187          |\n",
      "|    total_timesteps    | 696320       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0025985548 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.546        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0698      |\n",
      "|    mean_step_reward   | 0.015283527  |\n",
      "|    n_updates          | 336          |\n",
      "|    policyGradLoss     | -0.00373     |\n",
      "|    value_loss         | 0.0562       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 915          |\n",
      "|    iterations         | 22           |\n",
      "|    time_elapsed       | 196          |\n",
      "|    total_timesteps    | 704512       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0025358065 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | -0.823       |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.088       |\n",
      "|    mean_step_reward   | 0.010996567  |\n",
      "|    n_updates          | 340          |\n",
      "|    policyGradLoss     | -0.00282     |\n",
      "|    value_loss         | 0.0125       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 913          |\n",
      "|    iterations         | 23           |\n",
      "|    time_elapsed       | 206          |\n",
      "|    total_timesteps    | 712704       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0035728943 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.363        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0845      |\n",
      "|    mean_step_reward   | 0.011623202  |\n",
      "|    n_updates          | 344          |\n",
      "|    policyGradLoss     | -0.00457     |\n",
      "|    value_loss         | 0.0244       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 910         |\n",
      "|    iterations         | 24          |\n",
      "|    time_elapsed       | 215         |\n",
      "|    total_timesteps    | 720896      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002253914 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.396       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0675     |\n",
      "|    mean_step_reward   | 0.013839273 |\n",
      "|    n_updates          | 348         |\n",
      "|    policyGradLoss     | -0.00239    |\n",
      "|    value_loss         | 0.0444      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 908          |\n",
      "|    iterations         | 25           |\n",
      "|    time_elapsed       | 225          |\n",
      "|    total_timesteps    | 729088       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024903056 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.364        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.078       |\n",
      "|    mean_step_reward   | 0.014148377  |\n",
      "|    n_updates          | 352          |\n",
      "|    policyGradLoss     | -0.00281     |\n",
      "|    value_loss         | 0.0491       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 906          |\n",
      "|    iterations         | 26           |\n",
      "|    time_elapsed       | 235          |\n",
      "|    total_timesteps    | 737280       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0021600216 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.586        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0761      |\n",
      "|    mean_step_reward   | 0.013062862  |\n",
      "|    n_updates          | 356          |\n",
      "|    policyGradLoss     | -0.00316     |\n",
      "|    value_loss         | 0.0488       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 902          |\n",
      "|    iterations         | 27           |\n",
      "|    time_elapsed       | 244          |\n",
      "|    total_timesteps    | 745472       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0032938293 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.413        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0689      |\n",
      "|    mean_step_reward   | 0.012279399  |\n",
      "|    n_updates          | 360          |\n",
      "|    policyGradLoss     | -0.00277     |\n",
      "|    value_loss         | 0.0557       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 900          |\n",
      "|    iterations         | 28           |\n",
      "|    time_elapsed       | 254          |\n",
      "|    total_timesteps    | 753664       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0033836048 |\n",
      "|    entropy_loss       | -2.21        |\n",
      "|    explained_variance | 0.543        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0673      |\n",
      "|    mean_step_reward   | 0.012500828  |\n",
      "|    n_updates          | 364          |\n",
      "|    policyGradLoss     | -0.00334     |\n",
      "|    value_loss         | 0.045        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 898          |\n",
      "|    iterations         | 29           |\n",
      "|    time_elapsed       | 264          |\n",
      "|    total_timesteps    | 761856       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0028311764 |\n",
      "|    entropy_loss       | -2.21        |\n",
      "|    explained_variance | 0.607        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0617      |\n",
      "|    mean_step_reward   | 0.014149182  |\n",
      "|    n_updates          | 368          |\n",
      "|    policyGradLoss     | -0.00271     |\n",
      "|    value_loss         | 0.0592       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 897          |\n",
      "|    iterations         | 30           |\n",
      "|    time_elapsed       | 273          |\n",
      "|    total_timesteps    | 770048       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0030052008 |\n",
      "|    entropy_loss       | -2.2         |\n",
      "|    explained_variance | 0.516        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0642      |\n",
      "|    mean_step_reward   | 0.015656762  |\n",
      "|    n_updates          | 372          |\n",
      "|    policyGradLoss     | -0.00276     |\n",
      "|    value_loss         | 0.085        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 896          |\n",
      "|    iterations         | 31           |\n",
      "|    time_elapsed       | 283          |\n",
      "|    total_timesteps    | 778240       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026590931 |\n",
      "|    entropy_loss       | -2.21        |\n",
      "|    explained_variance | 0.57         |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0783      |\n",
      "|    mean_step_reward   | 0.013493736  |\n",
      "|    n_updates          | 376          |\n",
      "|    policyGradLoss     | -0.00365     |\n",
      "|    value_loss         | 0.0346       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 896          |\n",
      "|    iterations         | 32           |\n",
      "|    time_elapsed       | 292          |\n",
      "|    total_timesteps    | 786432       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026285315 |\n",
      "|    entropy_loss       | -2.22        |\n",
      "|    explained_variance | 0.55         |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0824      |\n",
      "|    mean_step_reward   | 0.012257995  |\n",
      "|    n_updates          | 380          |\n",
      "|    policyGradLoss     | -0.00233     |\n",
      "|    value_loss         | 0.0128       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SLR_2.zip\n",
      "[EVAL] Mean Return: -0.471, Best Return: -0.471\n",
      "Saved video to ./runs_smw/videos/SLR/SLR_2_-0.47.mp4\n",
      "\n",
      "=== Round 4 | Learn 262144 steps (Total trained: 786432) ===\n",
      "Logging to ./runs_smw/tb/SLR_0\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1401   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 5      |\n",
      "|    total_timesteps | 794624 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1096         |\n",
      "|    iterations         | 2            |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 802816       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0036720717 |\n",
      "|    entropy_loss       | -2.22        |\n",
      "|    explained_variance | 0.419        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0606      |\n",
      "|    mean_step_reward   | 0.013807568  |\n",
      "|    n_updates          | 388          |\n",
      "|    policyGradLoss     | -0.00302     |\n",
      "|    value_loss         | 0.0546       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1022        |\n",
      "|    iterations         | 3           |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 811008      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002821247 |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0.433       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0614     |\n",
      "|    mean_step_reward   | 0.013501913 |\n",
      "|    n_updates          | 392         |\n",
      "|    policyGradLoss     | -0.00308    |\n",
      "|    value_loss         | 0.053       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 986          |\n",
      "|    iterations         | 4            |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 819200       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0035648341 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.497        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0702      |\n",
      "|    mean_step_reward   | 0.013900235  |\n",
      "|    n_updates          | 396          |\n",
      "|    policyGradLoss     | -0.00285     |\n",
      "|    value_loss         | 0.0404       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 959          |\n",
      "|    iterations         | 5            |\n",
      "|    time_elapsed       | 42           |\n",
      "|    total_timesteps    | 827392       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026554563 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.491        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0898      |\n",
      "|    mean_step_reward   | 0.012333894  |\n",
      "|    n_updates          | 400          |\n",
      "|    policyGradLoss     | -0.0037      |\n",
      "|    value_loss         | 0.0261       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 942          |\n",
      "|    iterations         | 6            |\n",
      "|    time_elapsed       | 52           |\n",
      "|    total_timesteps    | 835584       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0018165897 |\n",
      "|    entropy_loss       | -2.23        |\n",
      "|    explained_variance | 0.571        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0815      |\n",
      "|    mean_step_reward   | 0.012514848  |\n",
      "|    n_updates          | 404          |\n",
      "|    policyGradLoss     | -0.00298     |\n",
      "|    value_loss         | 0.0248       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 930         |\n",
      "|    iterations         | 7           |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 843776      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002333036 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.502       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0776     |\n",
      "|    mean_step_reward   | 0.01324743  |\n",
      "|    n_updates          | 408         |\n",
      "|    policyGradLoss     | -0.00256    |\n",
      "|    value_loss         | 0.0404      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 921         |\n",
      "|    iterations         | 8           |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 851968      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.001893995 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.573       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.083      |\n",
      "|    mean_step_reward   | 0.011873714 |\n",
      "|    n_updates          | 412         |\n",
      "|    policyGradLoss     | -0.00306    |\n",
      "|    value_loss         | 0.0167      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 908          |\n",
      "|    iterations         | 9            |\n",
      "|    time_elapsed       | 81           |\n",
      "|    total_timesteps    | 860160       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0025705274 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.609        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0793      |\n",
      "|    mean_step_reward   | 0.013499617  |\n",
      "|    n_updates          | 416          |\n",
      "|    policyGradLoss     | -0.00356     |\n",
      "|    value_loss         | 0.0399       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 903         |\n",
      "|    iterations         | 10          |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 868352      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002647946 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.454       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0625     |\n",
      "|    mean_step_reward   | 0.014349544 |\n",
      "|    n_updates          | 420         |\n",
      "|    policyGradLoss     | -0.0029     |\n",
      "|    value_loss         | 0.0613      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 901          |\n",
      "|    iterations         | 11           |\n",
      "|    time_elapsed       | 99           |\n",
      "|    total_timesteps    | 876544       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0022867299 |\n",
      "|    entropy_loss       | -2.24        |\n",
      "|    explained_variance | 0.448        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.076       |\n",
      "|    mean_step_reward   | 0.013902694  |\n",
      "|    n_updates          | 424          |\n",
      "|    policyGradLoss     | -0.00393     |\n",
      "|    value_loss         | 0.042        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 899          |\n",
      "|    iterations         | 12           |\n",
      "|    time_elapsed       | 109          |\n",
      "|    total_timesteps    | 884736       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024096253 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.758        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0861      |\n",
      "|    mean_step_reward   | 0.014579419  |\n",
      "|    n_updates          | 428          |\n",
      "|    policyGradLoss     | -0.00432     |\n",
      "|    value_loss         | 0.0243       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 899         |\n",
      "|    iterations         | 13          |\n",
      "|    time_elapsed       | 118         |\n",
      "|    total_timesteps    | 892928      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002570277 |\n",
      "|    entropy_loss       | -2.27       |\n",
      "|    explained_variance | -0.668      |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0929     |\n",
      "|    mean_step_reward   | 0.011226742 |\n",
      "|    n_updates          | 432         |\n",
      "|    policyGradLoss     | -0.0031     |\n",
      "|    value_loss         | 0.0061      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 904         |\n",
      "|    iterations         | 14          |\n",
      "|    time_elapsed       | 126         |\n",
      "|    total_timesteps    | 901120      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002663552 |\n",
      "|    entropy_loss       | -2.26       |\n",
      "|    explained_variance | 0.507       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0898     |\n",
      "|    mean_step_reward   | 0.010667569 |\n",
      "|    n_updates          | 436         |\n",
      "|    policyGradLoss     | -0.00187    |\n",
      "|    value_loss         | 0.0173      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 904          |\n",
      "|    iterations         | 15           |\n",
      "|    time_elapsed       | 135          |\n",
      "|    total_timesteps    | 909312       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0029258449 |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.593        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0765      |\n",
      "|    mean_step_reward   | 0.012451817  |\n",
      "|    n_updates          | 440          |\n",
      "|    policyGradLoss     | -0.00357     |\n",
      "|    value_loss         | 0.021        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 902         |\n",
      "|    iterations         | 16          |\n",
      "|    time_elapsed       | 145         |\n",
      "|    total_timesteps    | 917504      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002969352 |\n",
      "|    entropy_loss       | -2.27       |\n",
      "|    explained_variance | 0.67        |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0783     |\n",
      "|    mean_step_reward   | 0.012683346 |\n",
      "|    n_updates          | 444         |\n",
      "|    policyGradLoss     | -0.00373    |\n",
      "|    value_loss         | 0.0283      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 902          |\n",
      "|    iterations         | 17           |\n",
      "|    time_elapsed       | 154          |\n",
      "|    total_timesteps    | 925696       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020757187 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.642        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0885      |\n",
      "|    mean_step_reward   | 0.012866168  |\n",
      "|    n_updates          | 448          |\n",
      "|    policyGradLoss     | -0.00403     |\n",
      "|    value_loss         | 0.0307       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 900          |\n",
      "|    iterations         | 18           |\n",
      "|    time_elapsed       | 163          |\n",
      "|    total_timesteps    | 933888       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0029968298 |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.607        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0832      |\n",
      "|    mean_step_reward   | 0.014261558  |\n",
      "|    n_updates          | 452          |\n",
      "|    policyGradLoss     | -0.00412     |\n",
      "|    value_loss         | 0.042        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 899          |\n",
      "|    iterations         | 19           |\n",
      "|    time_elapsed       | 173          |\n",
      "|    total_timesteps    | 942080       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0027108197 |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.508        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0894      |\n",
      "|    mean_step_reward   | 0.011482144  |\n",
      "|    n_updates          | 456          |\n",
      "|    policyGradLoss     | -0.00489     |\n",
      "|    value_loss         | 0.0172       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 899          |\n",
      "|    iterations         | 20           |\n",
      "|    time_elapsed       | 182          |\n",
      "|    total_timesteps    | 950272       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0033562086 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | -0.053       |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0871      |\n",
      "|    mean_step_reward   | 0.011137806  |\n",
      "|    n_updates          | 460          |\n",
      "|    policyGradLoss     | -0.00405     |\n",
      "|    value_loss         | 0.00639      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 900          |\n",
      "|    iterations         | 21           |\n",
      "|    time_elapsed       | 190          |\n",
      "|    total_timesteps    | 958464       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0030493839 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.517        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.082       |\n",
      "|    mean_step_reward   | 0.014009627  |\n",
      "|    n_updates          | 464          |\n",
      "|    policyGradLoss     | -0.00381     |\n",
      "|    value_loss         | 0.0333       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 903         |\n",
      "|    iterations         | 22          |\n",
      "|    time_elapsed       | 199         |\n",
      "|    total_timesteps    | 966656      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002352289 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.462       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.069      |\n",
      "|    mean_step_reward   | 0.012574844 |\n",
      "|    n_updates          | 468         |\n",
      "|    policyGradLoss     | -0.00433    |\n",
      "|    value_loss         | 0.0298      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 905          |\n",
      "|    iterations         | 23           |\n",
      "|    time_elapsed       | 207          |\n",
      "|    total_timesteps    | 974848       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0032321396 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.632        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0728      |\n",
      "|    mean_step_reward   | 0.014313346  |\n",
      "|    n_updates          | 472          |\n",
      "|    policyGradLoss     | -0.00482     |\n",
      "|    value_loss         | 0.0436       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 905         |\n",
      "|    iterations         | 24          |\n",
      "|    time_elapsed       | 217         |\n",
      "|    total_timesteps    | 983040      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002927249 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.477       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0776     |\n",
      "|    mean_step_reward   | 0.013462054 |\n",
      "|    n_updates          | 476         |\n",
      "|    policyGradLoss     | -0.00466    |\n",
      "|    value_loss         | 0.0456      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 902         |\n",
      "|    iterations         | 25          |\n",
      "|    time_elapsed       | 226         |\n",
      "|    total_timesteps    | 991232      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002835955 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.517       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0822     |\n",
      "|    mean_step_reward   | 0.01373142  |\n",
      "|    n_updates          | 480         |\n",
      "|    policyGradLoss     | -0.0043     |\n",
      "|    value_loss         | 0.0408      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 901          |\n",
      "|    iterations         | 26           |\n",
      "|    time_elapsed       | 236          |\n",
      "|    total_timesteps    | 999424       |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0023973319 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.651        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0917      |\n",
      "|    mean_step_reward   | 0.012450391  |\n",
      "|    n_updates          | 484          |\n",
      "|    policyGradLoss     | -0.00521     |\n",
      "|    value_loss         | 0.02         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 898          |\n",
      "|    iterations         | 27           |\n",
      "|    time_elapsed       | 246          |\n",
      "|    total_timesteps    | 1007616      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0024047303 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.739        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0811      |\n",
      "|    mean_step_reward   | 0.014211174  |\n",
      "|    n_updates          | 488          |\n",
      "|    policyGradLoss     | -0.00556     |\n",
      "|    value_loss         | 0.0364       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 895         |\n",
      "|    iterations         | 28          |\n",
      "|    time_elapsed       | 256         |\n",
      "|    total_timesteps    | 1015808     |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.00397262  |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.522       |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.08       |\n",
      "|    mean_step_reward   | 0.012704898 |\n",
      "|    n_updates          | 492         |\n",
      "|    policyGradLoss     | -0.00404    |\n",
      "|    value_loss         | 0.0411      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 892          |\n",
      "|    iterations         | 29           |\n",
      "|    time_elapsed       | 266          |\n",
      "|    total_timesteps    | 1024000      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0020191136 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.575        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0887      |\n",
      "|    mean_step_reward   | 0.012080644  |\n",
      "|    n_updates          | 496          |\n",
      "|    policyGradLoss     | -0.00496     |\n",
      "|    value_loss         | 0.0133       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 890          |\n",
      "|    iterations         | 30           |\n",
      "|    time_elapsed       | 275          |\n",
      "|    total_timesteps    | 1032192      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0028415415 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.581        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0808      |\n",
      "|    mean_step_reward   | 0.0137871625 |\n",
      "|    n_updates          | 500          |\n",
      "|    policyGradLoss     | -0.00415     |\n",
      "|    value_loss         | 0.0405       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 888          |\n",
      "|    iterations         | 31           |\n",
      "|    time_elapsed       | 285          |\n",
      "|    total_timesteps    | 1040384      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0030244244 |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | 0.641        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.084       |\n",
      "|    mean_step_reward   | 0.013320526  |\n",
      "|    n_updates          | 504          |\n",
      "|    policyGradLoss     | -0.00429     |\n",
      "|    value_loss         | 0.0381       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 886          |\n",
      "|    iterations         | 32           |\n",
      "|    time_elapsed       | 295          |\n",
      "|    total_timesteps    | 1048576      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0026001227 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.581        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0815      |\n",
      "|    mean_step_reward   | 0.012984172  |\n",
      "|    n_updates          | 508          |\n",
      "|    policyGradLoss     | -0.00399     |\n",
      "|    value_loss         | 0.0306       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SLR_3.zip\n",
      "[EVAL] Mean Return: -0.455, Best Return: -0.455\n",
      "Saved video to ./runs_smw/videos/SLR/SLR_3_-0.45.mp4\n",
      "\n",
      "=== Round 5 | Learn 262144 steps (Total trained: 1048576) ===\n",
      "Logging to ./runs_smw/tb/SLR_0\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1447    |\n",
      "|    iterations      | 1       |\n",
      "|    time_elapsed    | 5       |\n",
      "|    total_timesteps | 1056768 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1163         |\n",
      "|    iterations         | 2            |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 1064960      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0032050437 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.471        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0837      |\n",
      "|    mean_step_reward   | 0.01333954   |\n",
      "|    n_updates          | 516          |\n",
      "|    policyGradLoss     | -0.00336     |\n",
      "|    value_loss         | 0.0515       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1089         |\n",
      "|    iterations         | 3            |\n",
      "|    time_elapsed       | 22           |\n",
      "|    total_timesteps    | 1073152      |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0022497198 |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0.702        |\n",
      "|    learning_rate      | 0.0001       |\n",
      "|    loss               | -0.0797      |\n",
      "|    mean_step_reward   | 0.013993981  |\n",
      "|    n_updates          | 520          |\n",
      "|    policyGradLoss     | -0.00423     |\n",
      "|    value_loss         | 0.0447       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1055        |\n",
      "|    iterations         | 4           |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 1081344     |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002450689 |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.57        |\n",
      "|    learning_rate      | 0.0001      |\n",
      "|    loss               | -0.0795     |\n",
      "|    mean_step_reward   | 0.013262333 |\n",
      "|    n_updates          | 524         |\n",
      "|    policyGradLoss     | -0.00452    |\n",
      "|    value_loss         | 0.0314      |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while trained < TOTAL_STEPS:\n",
    "        round_idx += 1\n",
    "        chunk = min(TRAIN_CHUNK, TOTAL_STEPS - trained)\n",
    "        # chunk = 2000\n",
    "        label = \"SLR\" # super lose reward\n",
    "        tagged_label = f\"{label}_{int(trained/TRAIN_CHUNK)}\"\n",
    "\n",
    "        print(f\"\\n=== Round {round_idx} | Learn {chunk} steps (Total trained: {trained}) ===\")\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.learn(total_timesteps=chunk, reset_num_timesteps=False, tb_log_name=label)\n",
    "        trained += chunk\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"{tagged_label}.zip\")\n",
    "        model.save(ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        # --- Evaluate ---\n",
    "        mean_ret, best_ret = evaluate_policy(\n",
    "            model,\n",
    "            GAME,\n",
    "            STATE,\n",
    "            n_episodes=EVAL_EPISODES,\n",
    "            max_steps=EVAL_MAX_STEPS,\n",
    "        )\n",
    "        print(f\"[EVAL] Mean Return: {mean_ret:.3f}, Best Return: {best_ret:.3f}\")\n",
    "\n",
    "        # --- Save Best Model ---\n",
    "        # if mean_ret > best_mean:\n",
    "        #     best_mean = mean_ret\n",
    "        #     best_path = os.path.join(LOG_DIR, \"best_model.zip\")\n",
    "        #     model.save(best_path)\n",
    "        #     print(f\"New best record. Saved to {best_path}\")\n",
    "\n",
    "        # --- Record Video ---\n",
    "        out_path = os.path.join(VIDEO_DIR, label)\n",
    "        os.makedirs(out_path,  exist_ok=True)\n",
    "        record_video(\n",
    "            model,\n",
    "            GAME,\n",
    "            STATE,\n",
    "            VIDEO_DIR,\n",
    "            video_len=RECORD_STEPS,\n",
    "            prefix=f\"{label}/{tagged_label}_{mean_ret:.2f}\",\n",
    "        )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted manually.\")\n",
    "\n",
    "finally:\n",
    "    train_env.close()\n",
    "    print(\"Training finished. Environment closed.\")\n",
    "    \n",
    "\"\"\"\n",
    "tensorboard --logdir=./runs_smw/tb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088b0b3-2418-4866-b332-0312c9f6467f",
   "metadata": {},
   "source": [
    "## Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73191bb-e875-4939-b04c-a4670abd9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import glob\n",
    "\n",
    "list_of_files = glob.glob(os.path.join(VIDEO_DIR, '*.mp4')) \n",
    "if list_of_files:\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(f\"Playing: {latest_file}\")\n",
    "    display(Video(latest_file, embed=True, width=600))\n",
    "else:\n",
    "    print(\"No videos found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942adf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"runs_smw/videos/test_16.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Frame-by-Frame\", frame)\n",
    "\n",
    "    # 關鍵：這裡等待按鍵。按 'n' 鍵跳到下一幀，按 'q' 離開\n",
    "    key = cv2.waitKey(0) \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('n'):\n",
    "        continue\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab8 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
