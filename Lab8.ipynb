{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceedafe-2782-41a1-8119-50ee3d6c21fd",
   "metadata": {},
   "source": [
    "# 2025 DL Lab8: RL Assignment_Super Mario World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa555a-e61c-4fb3-b5d0-289b66570139",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm XXX, XXXXXXXXXX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b0974-9605-488a-9fd5-00816e7832cc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This project implements a **Deep Reinforcement Learning** pipeline to train an autonomous agent for Super Mario World. Leveraging the **Proximal Policy Optimization (PPO)** algorithm, the system interacts with the **stable-retro** environment to master the YoshiIsland1 level. Key components include a custom Vision Backbone for extracting features from raw pixel data and a suite of Environment Wrappers that handle frame preprocessing, action discretization, and reward shaping to facilitate efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02696447",
   "metadata": {},
   "source": [
    "Reward function implement  \n",
    "should do something in the beginning (monster attack)  \n",
    "Custom PPO implement  \n",
    "pre train weight 差不多，主要是 reward function  \n",
    "model weight capacity 1GB  \n",
    "class name 不要動 (可以新增，但是原本有的不要動)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a0ab9-f86d-4038-833d-761ec81fc4f2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b10def-362c-4910-9ed0-f3d0904343ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import retro\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "from eval import evaluate_policy, record_video\n",
    "from custom_policy import VisionBackbonePolicy, CustomPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10361fd-f291-4d93-b50d-cc749a3af588",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f6a25-738c-49dd-8e66-ae164b74a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Settings\n",
    "GAME = \"SuperMarioWorld-Snes\"\n",
    "STATE = \"YoshiIsland1\"\n",
    "\n",
    "# Training Settings\n",
    "TOTAL_STEPS = 6_553_600\n",
    "TRAIN_CHUNK =   327_680\n",
    "N_ENVS = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Evaluation & Recording Settings\n",
    "EVAL_EPISODES = 3\n",
    "EVAL_MAX_STEPS = 18000\n",
    "RECORD_STEPS = 1800\n",
    "\n",
    "# Directories\n",
    "LOG_DIR = \"./runs_smw\"\n",
    "VIDEO_DIR       = os.path.join(LOG_DIR, \"videos\")\n",
    "CKPT_DIR        = os.path.join(LOG_DIR, \"checkpoints\")\n",
    "TENSORBOARD_LOG = os.path.join(LOG_DIR, \"tb\")\n",
    "\n",
    "os.makedirs(LOG_DIR,   exist_ok=True)\n",
    "os.makedirs(CKPT_DIR,  exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34b783-0273-4835-ad2e-f9186064f76f",
   "metadata": {},
   "source": [
    "## Environment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c34213d-2c7c-42b8-922d-bafa285d1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import make_base_env\n",
    "def _make_env_thunk(game: str, state: str):\n",
    "    \"\"\"Return a function that creates an environment (for multiprocessing).\"\"\"\n",
    "    def _thunk():\n",
    "        return make_base_env(game, state)\n",
    "    return _thunk\n",
    "\n",
    "def make_vec_env(game: str, state: str, n_envs: int, use_subproc: bool = True):\n",
    "    \"\"\"Create a vectorized environment (multiple envs running in parallel).\"\"\"\n",
    "    env_fns = [_make_env_thunk(game, state) for _ in range(n_envs)]\n",
    "    \n",
    "    if use_subproc and n_envs > 1:\n",
    "        vec_env = SubprocVecEnv(env_fns)\n",
    "    else:\n",
    "        vec_env = DummyVecEnv(env_fns)\n",
    "\n",
    "    return vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dff476-ea2e-4262-8780-afb32ef1b233",
   "metadata": {},
   "source": [
    "## Initialize Env & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7c5fe-6421-4dbc-9bd8-822d61769c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created: SuperMarioWorld-Snes - YoshiIsland1 with 16 parallel envs.\n",
      "Fail to load None...\n",
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Training Environment\n",
    "train_env = make_vec_env(GAME, STATE, n_envs=N_ENVS)\n",
    "# train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)\n",
    "print(f\"Environment created: {GAME} - {STATE} with {N_ENVS} parallel envs.\")\n",
    "\n",
    "checkpoint_path = \"None\"\n",
    "# checkpoint_path = \"runs_smw/checkpoints/SF84_step_1600000.zip\"\n",
    "# checkpoint_path = \"runs_smw/checkpoints/SF84G_6553600.zip\"\n",
    "\n",
    "# 2. Initialize Model\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    # 讀取現有模型\n",
    "    model = CustomPPO.load(\n",
    "        checkpoint_path, \n",
    "        env=train_env,\n",
    "        device=\"cuda:0\" # 確保使用 GPU\n",
    "    )\n",
    "else:\n",
    "    print(f\"Fail to load {checkpoint_path}...\")\n",
    "    model = CustomPPO(\n",
    "        VisionBackbonePolicy,\n",
    "        train_env,\n",
    "        policy_kwargs   = dict(normalize_images=False),\n",
    "        n_epochs        = 4,\n",
    "        n_steps         = 512,\n",
    "        batch_size      = 512,\n",
    "        learning_rate   = LEARNING_RATE,\n",
    "        verbose         = 1,\n",
    "        gamma           = 0.99,\n",
    "        kl_coef         = 1,\n",
    "        clip_range      = 0.125,\n",
    "        tensorboard_log = TENSORBOARD_LOG,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594e443-843f-42c1-9fc6-3fbc82962021",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af4932-c531-4113-a33a-defc6fb5858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 | Learn 327680 steps (Total trained: 0) ===\n",
      "Logging to ./runs_smw/tb/SF84G_1220_0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/m314834001/Lab8/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 859  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 687          |\n",
      "|    iterations         | 2            |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 16384        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.09738616   |\n",
      "|    entropy_loss       | -2.44        |\n",
      "|    explained_variance | 0.0146       |\n",
      "|    learning_rate      | 0.00025      |\n",
      "|    loss               | -0.0611      |\n",
      "|    mean_step_reward   | -0.004130859 |\n",
      "|    n_updates          | 4            |\n",
      "|    policyGradLoss     | -0.0634      |\n",
      "|    value_loss         | 0.138        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 651         |\n",
      "|    iterations         | 3           |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 24576       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.033979207 |\n",
      "|    entropy_loss       | -2.34       |\n",
      "|    explained_variance | 0.147       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0177     |\n",
      "|    mean_step_reward   | 0.029892579 |\n",
      "|    n_updates          | 8           |\n",
      "|    policyGradLoss     | -0.0104     |\n",
      "|    value_loss         | 0.188       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 637          |\n",
      "|    iterations         | 4            |\n",
      "|    time_elapsed       | 51           |\n",
      "|    total_timesteps    | 32768        |\n",
      "| train/                |              |\n",
      "|    approx_kl          | 0.0076069925 |\n",
      "|    entropy_loss       | -2.21        |\n",
      "|    explained_variance | 0.582        |\n",
      "|    learning_rate      | 0.00025      |\n",
      "|    loss               | 0.0141       |\n",
      "|    mean_step_reward   | 0.042600103  |\n",
      "|    n_updates          | 12           |\n",
      "|    policyGradLoss     | -0.00289     |\n",
      "|    value_loss         | 0.252        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 5           |\n",
      "|    time_elapsed       | 65          |\n",
      "|    total_timesteps    | 40960       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.008078446 |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0.798       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0125     |\n",
      "|    mean_step_reward   | 0.042500004 |\n",
      "|    n_updates          | 16          |\n",
      "|    policyGradLoss     | -0.00193    |\n",
      "|    value_loss         | 0.183       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 615         |\n",
      "|    iterations         | 6           |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 49152       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.007344107 |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | 0.88        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0286      |\n",
      "|    mean_step_reward   | 0.040051274 |\n",
      "|    n_updates          | 20          |\n",
      "|    policyGradLoss     | -0.00183    |\n",
      "|    value_loss         | 0.229       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 611         |\n",
      "|    iterations         | 7           |\n",
      "|    time_elapsed       | 93          |\n",
      "|    total_timesteps    | 57344       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.013173107 |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0.869       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0538      |\n",
      "|    mean_step_reward   | 0.050705574 |\n",
      "|    n_updates          | 24          |\n",
      "|    policyGradLoss     | -0.00344    |\n",
      "|    value_loss         | 0.336       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 605         |\n",
      "|    iterations         | 8           |\n",
      "|    time_elapsed       | 108         |\n",
      "|    total_timesteps    | 65536       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.002995147 |\n",
      "|    entropy_loss       | -2.05       |\n",
      "|    explained_variance | 0.912       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0516      |\n",
      "|    mean_step_reward   | 0.04679444  |\n",
      "|    n_updates          | 28          |\n",
      "|    policyGradLoss     | -0.000408   |\n",
      "|    value_loss         | 0.284       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 602         |\n",
      "|    iterations         | 9           |\n",
      "|    time_elapsed       | 122         |\n",
      "|    total_timesteps    | 73728       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.005555776 |\n",
      "|    entropy_loss       | -2.05       |\n",
      "|    explained_variance | 0.892       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.108       |\n",
      "|    mean_step_reward   | 0.04412598  |\n",
      "|    n_updates          | 32          |\n",
      "|    policyGradLoss     | -0.000862   |\n",
      "|    value_loss         | 0.349       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 600         |\n",
      "|    iterations         | 10          |\n",
      "|    time_elapsed       | 136         |\n",
      "|    total_timesteps    | 81920       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.02035505  |\n",
      "|    entropy_loss       | -2.08       |\n",
      "|    explained_variance | 0.92        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0434      |\n",
      "|    mean_step_reward   | 0.040556647 |\n",
      "|    n_updates          | 36          |\n",
      "|    policyGradLoss     | -0.00287    |\n",
      "|    value_loss         | 0.32        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 597         |\n",
      "|    iterations         | 11          |\n",
      "|    time_elapsed       | 150         |\n",
      "|    total_timesteps    | 90112       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022662856 |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 0.913       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0578      |\n",
      "|    mean_step_reward   | 0.04248536  |\n",
      "|    n_updates          | 40          |\n",
      "|    policyGradLoss     | -0.0014     |\n",
      "|    value_loss         | 0.323       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 12          |\n",
      "|    time_elapsed       | 164         |\n",
      "|    total_timesteps    | 98304       |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.021163695 |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0.95        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.00211     |\n",
      "|    mean_step_reward   | 0.038325198 |\n",
      "|    n_updates          | 44          |\n",
      "|    policyGradLoss     | -0.00293    |\n",
      "|    value_loss         | 0.186       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 595        |\n",
      "|    iterations         | 13         |\n",
      "|    time_elapsed       | 178        |\n",
      "|    total_timesteps    | 106496     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.01771947 |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | 0.953      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | -0.0311    |\n",
      "|    mean_step_reward   | 0.0372168  |\n",
      "|    n_updates          | 48         |\n",
      "|    policyGradLoss     | -0.00493   |\n",
      "|    value_loss         | 0.117      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 14          |\n",
      "|    time_elapsed       | 192         |\n",
      "|    total_timesteps    | 114688      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.025669955 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.976       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0792     |\n",
      "|    mean_step_reward   | 0.041770026 |\n",
      "|    n_updates          | 52          |\n",
      "|    policyGradLoss     | -0.00882    |\n",
      "|    value_loss         | 0.079       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 15          |\n",
      "|    time_elapsed       | 207         |\n",
      "|    total_timesteps    | 122880      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022216571 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.962       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.028      |\n",
      "|    mean_step_reward   | 0.038293462 |\n",
      "|    n_updates          | 56          |\n",
      "|    policyGradLoss     | -0.00303    |\n",
      "|    value_loss         | 0.115       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 16          |\n",
      "|    time_elapsed       | 221         |\n",
      "|    total_timesteps    | 131072      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022986999 |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0.933       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0115     |\n",
      "|    mean_step_reward   | 0.04340577  |\n",
      "|    n_updates          | 60          |\n",
      "|    policyGradLoss     | -0.00794    |\n",
      "|    value_loss         | 0.201       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 591        |\n",
      "|    iterations         | 17         |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 139264     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.01899106 |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0.906      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0738     |\n",
      "|    mean_step_reward   | 0.04440186 |\n",
      "|    n_updates          | 64         |\n",
      "|    policyGradLoss     | -0.00545   |\n",
      "|    value_loss         | 0.289      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 18          |\n",
      "|    time_elapsed       | 249         |\n",
      "|    total_timesteps    | 147456      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.008310229 |\n",
      "|    entropy_loss       | -2.07       |\n",
      "|    explained_variance | 0.835       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.168       |\n",
      "|    mean_step_reward   | 0.04830811  |\n",
      "|    n_updates          | 68          |\n",
      "|    policyGradLoss     | -0.00258    |\n",
      "|    value_loss         | 0.532       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 19          |\n",
      "|    time_elapsed       | 263         |\n",
      "|    total_timesteps    | 155648      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.02760832  |\n",
      "|    entropy_loss       | -2.14       |\n",
      "|    explained_variance | 0.893       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.116       |\n",
      "|    mean_step_reward   | 0.045944832 |\n",
      "|    n_updates          | 72          |\n",
      "|    policyGradLoss     | -0.00171    |\n",
      "|    value_loss         | 0.444       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 20          |\n",
      "|    time_elapsed       | 277         |\n",
      "|    total_timesteps    | 163840      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.024586324 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.941       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.00456    |\n",
      "|    mean_step_reward   | 0.041125495 |\n",
      "|    n_updates          | 76          |\n",
      "|    policyGradLoss     | -0.00487    |\n",
      "|    value_loss         | 0.166       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 21          |\n",
      "|    time_elapsed       | 292         |\n",
      "|    total_timesteps    | 172032      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.018836314 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.92        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0316      |\n",
      "|    mean_step_reward   | 0.036687016 |\n",
      "|    n_updates          | 80          |\n",
      "|    policyGradLoss     | -0.00362    |\n",
      "|    value_loss         | 0.245       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 22          |\n",
      "|    time_elapsed       | 306         |\n",
      "|    total_timesteps    | 180224      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.015430287 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.916       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0432      |\n",
      "|    mean_step_reward   | 0.04318116  |\n",
      "|    n_updates          | 84          |\n",
      "|    policyGradLoss     | -0.00881    |\n",
      "|    value_loss         | 0.246       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 23          |\n",
      "|    time_elapsed       | 320         |\n",
      "|    total_timesteps    | 188416      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.018870965 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.929       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0418      |\n",
      "|    mean_step_reward   | 0.044133306 |\n",
      "|    n_updates          | 88          |\n",
      "|    policyGradLoss     | -0.00808    |\n",
      "|    value_loss         | 0.253       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 587         |\n",
      "|    iterations         | 24          |\n",
      "|    time_elapsed       | 334         |\n",
      "|    total_timesteps    | 196608      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.023792176 |\n",
      "|    entropy_loss       | -2.26       |\n",
      "|    explained_variance | 0.914       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0109     |\n",
      "|    mean_step_reward   | 0.039526373 |\n",
      "|    n_updates          | 92          |\n",
      "|    policyGradLoss     | -0.0145     |\n",
      "|    value_loss         | 0.254       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 25          |\n",
      "|    time_elapsed       | 348         |\n",
      "|    total_timesteps    | 204800      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.02762703  |\n",
      "|    entropy_loss       | -2.25       |\n",
      "|    explained_variance | 0.943       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0291     |\n",
      "|    mean_step_reward   | 0.036909185 |\n",
      "|    n_updates          | 96          |\n",
      "|    policyGradLoss     | -0.0102     |\n",
      "|    value_loss         | 0.163       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 26          |\n",
      "|    time_elapsed       | 362         |\n",
      "|    total_timesteps    | 212992      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022115063 |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.95        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.00163     |\n",
      "|    mean_step_reward   | 0.040515143 |\n",
      "|    n_updates          | 100         |\n",
      "|    policyGradLoss     | -0.0117     |\n",
      "|    value_loss         | 0.161       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 587         |\n",
      "|    iterations         | 27          |\n",
      "|    time_elapsed       | 376         |\n",
      "|    total_timesteps    | 221184      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.029313406 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.887       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0449      |\n",
      "|    mean_step_reward   | 0.043562017 |\n",
      "|    n_updates          | 104         |\n",
      "|    policyGradLoss     | -0.0133     |\n",
      "|    value_loss         | 0.312       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 587         |\n",
      "|    iterations         | 28          |\n",
      "|    time_elapsed       | 390         |\n",
      "|    total_timesteps    | 229376      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.042145684 |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0.876       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0187     |\n",
      "|    mean_step_reward   | 0.030187992 |\n",
      "|    n_updates          | 108         |\n",
      "|    policyGradLoss     | -0.0194     |\n",
      "|    value_loss         | 0.265       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 29          |\n",
      "|    time_elapsed       | 402         |\n",
      "|    total_timesteps    | 237568      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.02825841  |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.833       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0455      |\n",
      "|    mean_step_reward   | 0.039438482 |\n",
      "|    n_updates          | 112         |\n",
      "|    policyGradLoss     | -0.0154     |\n",
      "|    value_loss         | 0.382       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 30          |\n",
      "|    time_elapsed       | 415         |\n",
      "|    total_timesteps    | 245760      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.018167598 |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0.875       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.117       |\n",
      "|    mean_step_reward   | 0.04552979  |\n",
      "|    n_updates          | 116         |\n",
      "|    policyGradLoss     | -0.00887    |\n",
      "|    value_loss         | 0.356       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 31          |\n",
      "|    time_elapsed       | 428         |\n",
      "|    total_timesteps    | 253952      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022592653 |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0.897       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0273      |\n",
      "|    mean_step_reward   | 0.048728034 |\n",
      "|    n_updates          | 120         |\n",
      "|    policyGradLoss     | -0.00947    |\n",
      "|    value_loss         | 0.403       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 32          |\n",
      "|    time_elapsed       | 441         |\n",
      "|    total_timesteps    | 262144      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.023727939 |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | 0.89        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.143       |\n",
      "|    mean_step_reward   | 0.047165535 |\n",
      "|    n_updates          | 124         |\n",
      "|    policyGradLoss     | -0.0127     |\n",
      "|    value_loss         | 0.452       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 595         |\n",
      "|    iterations         | 33          |\n",
      "|    time_elapsed       | 453         |\n",
      "|    total_timesteps    | 270336      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022052692 |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0.84        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.228       |\n",
      "|    mean_step_reward   | 0.04712403  |\n",
      "|    n_updates          | 128         |\n",
      "|    policyGradLoss     | -0.0133     |\n",
      "|    value_loss         | 0.705       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 598         |\n",
      "|    iterations         | 34          |\n",
      "|    time_elapsed       | 465         |\n",
      "|    total_timesteps    | 278528      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.022484172 |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 0.873       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.122       |\n",
      "|    mean_step_reward   | 0.045952156 |\n",
      "|    n_updates          | 132         |\n",
      "|    policyGradLoss     | -0.0116     |\n",
      "|    value_loss         | 0.51        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 602         |\n",
      "|    iterations         | 35          |\n",
      "|    time_elapsed       | 475         |\n",
      "|    total_timesteps    | 286720      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.020104181 |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0.873       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.157       |\n",
      "|    mean_step_reward   | 0.046364754 |\n",
      "|    n_updates          | 136         |\n",
      "|    policyGradLoss     | -0.0122     |\n",
      "|    value_loss         | 0.623       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 605         |\n",
      "|    iterations         | 36          |\n",
      "|    time_elapsed       | 486         |\n",
      "|    total_timesteps    | 294912      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.032273382 |\n",
      "|    entropy_loss       | -2.24       |\n",
      "|    explained_variance | 0.856       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0827      |\n",
      "|    mean_step_reward   | 0.04059571  |\n",
      "|    n_updates          | 140         |\n",
      "|    policyGradLoss     | -0.0179     |\n",
      "|    value_loss         | 0.54        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 606        |\n",
      "|    iterations         | 37         |\n",
      "|    time_elapsed       | 499        |\n",
      "|    total_timesteps    | 303104     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.04226541 |\n",
      "|    entropy_loss       | -2.24      |\n",
      "|    explained_variance | 0.924      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0378     |\n",
      "|    mean_step_reward   | 0.04212159 |\n",
      "|    n_updates          | 144        |\n",
      "|    policyGradLoss     | -0.0202    |\n",
      "|    value_loss         | 0.285      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 605         |\n",
      "|    iterations         | 38          |\n",
      "|    time_elapsed       | 514         |\n",
      "|    total_timesteps    | 311296      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.03872565  |\n",
      "|    entropy_loss       | -2.23       |\n",
      "|    explained_variance | 0.898       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0277      |\n",
      "|    mean_step_reward   | 0.036979984 |\n",
      "|    n_updates          | 148         |\n",
      "|    policyGradLoss     | -0.0184     |\n",
      "|    value_loss         | 0.348       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 604        |\n",
      "|    iterations         | 39         |\n",
      "|    time_elapsed       | 528        |\n",
      "|    total_timesteps    | 319488     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.03511913 |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0.865      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0727     |\n",
      "|    mean_step_reward   | 0.03969971 |\n",
      "|    n_updates          | 152        |\n",
      "|    policyGradLoss     | -0.0199    |\n",
      "|    value_loss         | 0.446      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 603         |\n",
      "|    iterations         | 40          |\n",
      "|    time_elapsed       | 542         |\n",
      "|    total_timesteps    | 327680      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.04835567  |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0.877       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0466      |\n",
      "|    mean_step_reward   | 0.030068364 |\n",
      "|    n_updates          | 156         |\n",
      "|    policyGradLoss     | -0.0212     |\n",
      "|    value_loss         | 0.365       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SF84G_1.zip\n",
      "[EVAL] Mean Return: -0.310, Best Return: -0.310\n",
      "New best record. Saved to ./runs_smw/best_model.zip\n",
      "Saved video to ./runs_smw/videos/step_327680_mean_-0.31.mp4\n",
      "\n",
      "=== Round 2 | Learn 327680 steps (Total trained: 327680) ===\n",
      "Logging to ./runs_smw/tb/SF84G_1220_0\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 752    |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 10     |\n",
      "|    total_timesteps | 335872 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 663         |\n",
      "|    iterations         | 2           |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 344064      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.049360733 |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0.856       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.113       |\n",
      "|    mean_step_reward   | 0.030712891 |\n",
      "|    n_updates          | 164         |\n",
      "|    policyGradLoss     | -0.0242     |\n",
      "|    value_loss         | 0.448       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 635         |\n",
      "|    iterations         | 3           |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 352256      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.04302212  |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0.893       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0497      |\n",
      "|    mean_step_reward   | 0.034799807 |\n",
      "|    n_updates          | 168         |\n",
      "|    policyGradLoss     | -0.0159     |\n",
      "|    value_loss         | 0.365       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 622         |\n",
      "|    iterations         | 4           |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 360448      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06639819  |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0.834       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0127      |\n",
      "|    mean_step_reward   | 0.027983401 |\n",
      "|    n_updates          | 172         |\n",
      "|    policyGradLoss     | -0.0295     |\n",
      "|    value_loss         | 0.359       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 614         |\n",
      "|    iterations         | 5           |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 368640      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.060005244 |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0.867       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0257      |\n",
      "|    mean_step_reward   | 0.03200684  |\n",
      "|    n_updates          | 176         |\n",
      "|    policyGradLoss     | -0.0306     |\n",
      "|    value_loss         | 0.328       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 609         |\n",
      "|    iterations         | 6           |\n",
      "|    time_elapsed       | 80          |\n",
      "|    total_timesteps    | 376832      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.05316694  |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | 0.906       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.011       |\n",
      "|    mean_step_reward   | 0.036860358 |\n",
      "|    n_updates          | 180         |\n",
      "|    policyGradLoss     | -0.0212     |\n",
      "|    value_loss         | 0.26        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 605         |\n",
      "|    iterations         | 7           |\n",
      "|    time_elapsed       | 94          |\n",
      "|    total_timesteps    | 385024      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.051483687 |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0.849       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0257      |\n",
      "|    mean_step_reward   | 0.045146488 |\n",
      "|    n_updates          | 184         |\n",
      "|    policyGradLoss     | -0.0195     |\n",
      "|    value_loss         | 0.395       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 601         |\n",
      "|    iterations         | 8           |\n",
      "|    time_elapsed       | 108         |\n",
      "|    total_timesteps    | 393216      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.047338046 |\n",
      "|    entropy_loss       | -2.1        |\n",
      "|    explained_variance | 0.87        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0422      |\n",
      "|    mean_step_reward   | 0.0434253   |\n",
      "|    n_updates          | 188         |\n",
      "|    policyGradLoss     | -0.0207     |\n",
      "|    value_loss         | 0.394       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 598         |\n",
      "|    iterations         | 9           |\n",
      "|    time_elapsed       | 123         |\n",
      "|    total_timesteps    | 401408      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06434986  |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0.897       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.038       |\n",
      "|    mean_step_reward   | 0.038767092 |\n",
      "|    n_updates          | 192         |\n",
      "|    policyGradLoss     | -0.0338     |\n",
      "|    value_loss         | 0.308       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 595         |\n",
      "|    iterations         | 10          |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 409600      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06987276  |\n",
      "|    entropy_loss       | -2.09       |\n",
      "|    explained_variance | 0.884       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.000177    |\n",
      "|    mean_step_reward   | 0.044958502 |\n",
      "|    n_updates          | 196         |\n",
      "|    policyGradLoss     | -0.0321     |\n",
      "|    value_loss         | 0.329       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 11          |\n",
      "|    time_elapsed       | 151         |\n",
      "|    total_timesteps    | 417792      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06602831  |\n",
      "|    entropy_loss       | -2.12       |\n",
      "|    explained_variance | 0.864       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0357      |\n",
      "|    mean_step_reward   | 0.039187014 |\n",
      "|    n_updates          | 200         |\n",
      "|    policyGradLoss     | -0.0318     |\n",
      "|    value_loss         | 0.34        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 12          |\n",
      "|    time_elapsed       | 165         |\n",
      "|    total_timesteps    | 425984      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.0786099   |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0.889       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0443      |\n",
      "|    mean_step_reward   | 0.036779787 |\n",
      "|    n_updates          | 204         |\n",
      "|    policyGradLoss     | -0.023      |\n",
      "|    value_loss         | 0.347       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 13          |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 434176      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.056057494 |\n",
      "|    entropy_loss       | -2.1        |\n",
      "|    explained_variance | 0.901       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0169      |\n",
      "|    mean_step_reward   | 0.037812505 |\n",
      "|    n_updates          | 208         |\n",
      "|    policyGradLoss     | -0.0196     |\n",
      "|    value_loss         | 0.261       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 14          |\n",
      "|    time_elapsed       | 193         |\n",
      "|    total_timesteps    | 442368      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.048279554 |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0.909       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0236      |\n",
      "|    mean_step_reward   | 0.041721195 |\n",
      "|    n_updates          | 212         |\n",
      "|    policyGradLoss     | -0.0184     |\n",
      "|    value_loss         | 0.251       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 15          |\n",
      "|    time_elapsed       | 207         |\n",
      "|    total_timesteps    | 450560      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.056740593 |\n",
      "|    entropy_loss       | -2.14       |\n",
      "|    explained_variance | 0.904       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0139      |\n",
      "|    mean_step_reward   | 0.030642092 |\n",
      "|    n_updates          | 216         |\n",
      "|    policyGradLoss     | -0.0186     |\n",
      "|    value_loss         | 0.277       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 16          |\n",
      "|    time_elapsed       | 221         |\n",
      "|    total_timesteps    | 458752      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.053292222 |\n",
      "|    entropy_loss       | -2.12       |\n",
      "|    explained_variance | 0.889       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.00112     |\n",
      "|    mean_step_reward   | 0.033232428 |\n",
      "|    n_updates          | 220         |\n",
      "|    policyGradLoss     | -0.0308     |\n",
      "|    value_loss         | 0.291       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 17          |\n",
      "|    time_elapsed       | 235         |\n",
      "|    total_timesteps    | 466944      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.047387455 |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0.854       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.103       |\n",
      "|    mean_step_reward   | 0.045543216 |\n",
      "|    n_updates          | 224         |\n",
      "|    policyGradLoss     | -0.0284     |\n",
      "|    value_loss         | 0.419       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 18          |\n",
      "|    time_elapsed       | 250         |\n",
      "|    total_timesteps    | 475136      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.042693615 |\n",
      "|    entropy_loss       | -2.05       |\n",
      "|    explained_variance | 0.891       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0618      |\n",
      "|    mean_step_reward   | 0.04818848  |\n",
      "|    n_updates          | 228         |\n",
      "|    policyGradLoss     | -0.0255     |\n",
      "|    value_loss         | 0.388       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 19          |\n",
      "|    time_elapsed       | 264         |\n",
      "|    total_timesteps    | 483328      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.045788296 |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0.909       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0057      |\n",
      "|    mean_step_reward   | 0.040517583 |\n",
      "|    n_updates          | 232         |\n",
      "|    policyGradLoss     | -0.0237     |\n",
      "|    value_loss         | 0.279       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 20          |\n",
      "|    time_elapsed       | 278         |\n",
      "|    total_timesteps    | 491520      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.051819842 |\n",
      "|    entropy_loss       | -2.02       |\n",
      "|    explained_variance | 0.883       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.102       |\n",
      "|    mean_step_reward   | 0.043803714 |\n",
      "|    n_updates          | 236         |\n",
      "|    policyGradLoss     | -0.0196     |\n",
      "|    value_loss         | 0.389       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 21          |\n",
      "|    time_elapsed       | 291         |\n",
      "|    total_timesteps    | 499712      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.0426998   |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.898       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0683      |\n",
      "|    mean_step_reward   | 0.046628427 |\n",
      "|    n_updates          | 240         |\n",
      "|    policyGradLoss     | -0.0192     |\n",
      "|    value_loss         | 0.444       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 22          |\n",
      "|    time_elapsed       | 306         |\n",
      "|    total_timesteps    | 507904      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.049664248 |\n",
      "|    entropy_loss       | -1.98       |\n",
      "|    explained_variance | 0.878       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0985      |\n",
      "|    mean_step_reward   | 0.049008798 |\n",
      "|    n_updates          | 244         |\n",
      "|    policyGradLoss     | -0.0235     |\n",
      "|    value_loss         | 0.503       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 23          |\n",
      "|    time_elapsed       | 319         |\n",
      "|    total_timesteps    | 516096      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.052271977 |\n",
      "|    entropy_loss       | -1.98       |\n",
      "|    explained_variance | 0.848       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.156       |\n",
      "|    mean_step_reward   | 0.04593751  |\n",
      "|    n_updates          | 248         |\n",
      "|    policyGradLoss     | -0.0279     |\n",
      "|    value_loss         | 0.745       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 592        |\n",
      "|    iterations         | 24         |\n",
      "|    time_elapsed       | 331        |\n",
      "|    total_timesteps    | 524288     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.05145664 |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 0.886      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.154      |\n",
      "|    mean_step_reward   | 0.05291993 |\n",
      "|    n_updates          | 252        |\n",
      "|    policyGradLoss     | -0.0215    |\n",
      "|    value_loss         | 0.66       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 25          |\n",
      "|    time_elapsed       | 344         |\n",
      "|    total_timesteps    | 532480      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.051002245 |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 0.891       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0502      |\n",
      "|    mean_step_reward   | 0.045319833 |\n",
      "|    n_updates          | 256         |\n",
      "|    policyGradLoss     | -0.00687    |\n",
      "|    value_loss         | 0.4         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 26          |\n",
      "|    time_elapsed       | 357         |\n",
      "|    total_timesteps    | 540672      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.039167747 |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.886       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.116       |\n",
      "|    mean_step_reward   | 0.047417    |\n",
      "|    n_updates          | 260         |\n",
      "|    policyGradLoss     | -0.0134     |\n",
      "|    value_loss         | 0.537       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 27          |\n",
      "|    time_elapsed       | 370         |\n",
      "|    total_timesteps    | 548864      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.051037572 |\n",
      "|    entropy_loss       | -2.03       |\n",
      "|    explained_variance | 0.86        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.126       |\n",
      "|    mean_step_reward   | 0.044794925 |\n",
      "|    n_updates          | 264         |\n",
      "|    policyGradLoss     | -0.0233     |\n",
      "|    value_loss         | 0.62        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 28          |\n",
      "|    time_elapsed       | 384         |\n",
      "|    total_timesteps    | 557056      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.058822427 |\n",
      "|    entropy_loss       | -2.03       |\n",
      "|    explained_variance | 0.842       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.16        |\n",
      "|    mean_step_reward   | 0.045124516 |\n",
      "|    n_updates          | 268         |\n",
      "|    policyGradLoss     | -0.023      |\n",
      "|    value_loss         | 0.745       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 595         |\n",
      "|    iterations         | 29          |\n",
      "|    time_elapsed       | 398         |\n",
      "|    total_timesteps    | 565248      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.07057248  |\n",
      "|    entropy_loss       | -2          |\n",
      "|    explained_variance | 0.867       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.109       |\n",
      "|    mean_step_reward   | 0.050915536 |\n",
      "|    n_updates          | 272         |\n",
      "|    policyGradLoss     | -0.0301     |\n",
      "|    value_loss         | 0.656       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 594        |\n",
      "|    iterations         | 30         |\n",
      "|    time_elapsed       | 413        |\n",
      "|    total_timesteps    | 573440     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.05592016 |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 0.862      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0709     |\n",
      "|    mean_step_reward   | 0.04570069 |\n",
      "|    n_updates          | 276        |\n",
      "|    policyGradLoss     | -0.0258    |\n",
      "|    value_loss         | 0.568      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 594        |\n",
      "|    iterations         | 31         |\n",
      "|    time_elapsed       | 427        |\n",
      "|    total_timesteps    | 581632     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.07955406 |\n",
      "|    entropy_loss       | -2         |\n",
      "|    explained_variance | 0.856      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.274      |\n",
      "|    mean_step_reward   | 0.05427979 |\n",
      "|    n_updates          | 280        |\n",
      "|    policyGradLoss     | -0.0254    |\n",
      "|    value_loss         | 0.714      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 32          |\n",
      "|    time_elapsed       | 441         |\n",
      "|    total_timesteps    | 589824      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.1215977   |\n",
      "|    entropy_loss       | -2.04       |\n",
      "|    explained_variance | 0.823       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.136       |\n",
      "|    mean_step_reward   | 0.037670903 |\n",
      "|    n_updates          | 284         |\n",
      "|    policyGradLoss     | -0.0194     |\n",
      "|    value_loss         | 0.625       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 33          |\n",
      "|    time_elapsed       | 455         |\n",
      "|    total_timesteps    | 598016      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.05107435  |\n",
      "|    entropy_loss       | -2          |\n",
      "|    explained_variance | 0.882       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0966      |\n",
      "|    mean_step_reward   | 0.050360113 |\n",
      "|    n_updates          | 288         |\n",
      "|    policyGradLoss     | -0.0196     |\n",
      "|    value_loss         | 0.58        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 34          |\n",
      "|    time_elapsed       | 470         |\n",
      "|    total_timesteps    | 606208      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.061569214 |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0.886       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.128       |\n",
      "|    mean_step_reward   | 0.05035645  |\n",
      "|    n_updates          | 292         |\n",
      "|    policyGradLoss     | -0.0238     |\n",
      "|    value_loss         | 0.629       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 35          |\n",
      "|    time_elapsed       | 484         |\n",
      "|    total_timesteps    | 614400      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.070405334 |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0.876       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.126       |\n",
      "|    mean_step_reward   | 0.04806397  |\n",
      "|    n_updates          | 296         |\n",
      "|    policyGradLoss     | -0.0204     |\n",
      "|    value_loss         | 0.576       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 591         |\n",
      "|    iterations         | 36          |\n",
      "|    time_elapsed       | 498         |\n",
      "|    total_timesteps    | 622592      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.061863385 |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0.891       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.133       |\n",
      "|    mean_step_reward   | 0.0475586   |\n",
      "|    n_updates          | 300         |\n",
      "|    policyGradLoss     | -0.0302     |\n",
      "|    value_loss         | 0.593       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 37          |\n",
      "|    time_elapsed       | 513         |\n",
      "|    total_timesteps    | 630784      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.059652533 |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 0.899       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0833      |\n",
      "|    mean_step_reward   | 0.046179205 |\n",
      "|    n_updates          | 304         |\n",
      "|    policyGradLoss     | -0.0242     |\n",
      "|    value_loss         | 0.588       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 38          |\n",
      "|    time_elapsed       | 527         |\n",
      "|    total_timesteps    | 638976      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06941579  |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.906       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0718      |\n",
      "|    mean_step_reward   | 0.044443365 |\n",
      "|    n_updates          | 308         |\n",
      "|    policyGradLoss     | -0.0314     |\n",
      "|    value_loss         | 0.519       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 39          |\n",
      "|    time_elapsed       | 541         |\n",
      "|    total_timesteps    | 647168      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.061323456 |\n",
      "|    entropy_loss       | -2          |\n",
      "|    explained_variance | 0.901       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0965      |\n",
      "|    mean_step_reward   | 0.04791016  |\n",
      "|    n_updates          | 312         |\n",
      "|    policyGradLoss     | -0.0394     |\n",
      "|    value_loss         | 0.543       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 40          |\n",
      "|    time_elapsed       | 555         |\n",
      "|    total_timesteps    | 655360      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06284907  |\n",
      "|    entropy_loss       | -2.02       |\n",
      "|    explained_variance | 0.881       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0878      |\n",
      "|    mean_step_reward   | 0.040952154 |\n",
      "|    n_updates          | 316         |\n",
      "|    policyGradLoss     | -0.0333     |\n",
      "|    value_loss         | 0.594       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./runs_smw/checkpoints/SF84G_2.zip\n",
      "[EVAL] Mean Return: 6.860, Best Return: 6.860\n",
      "New best record. Saved to ./runs_smw/best_model.zip\n",
      "Saved video to ./runs_smw/videos/step_655360_mean_6.86.mp4\n",
      "\n",
      "=== Round 3 | Learn 327680 steps (Total trained: 655360) ===\n",
      "Logging to ./runs_smw/tb/SF84G_1220_0\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 725    |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 11     |\n",
      "|    total_timesteps | 663552 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 645         |\n",
      "|    iterations         | 2           |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 671744      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.09827865  |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.866       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0103      |\n",
      "|    mean_step_reward   | 0.038579106 |\n",
      "|    n_updates          | 324         |\n",
      "|    policyGradLoss     | -0.0338     |\n",
      "|    value_loss         | 0.544       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 620         |\n",
      "|    iterations         | 3           |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 679936      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.12317516  |\n",
      "|    entropy_loss       | -2.02       |\n",
      "|    explained_variance | 0.867       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0251     |\n",
      "|    mean_step_reward   | 0.032749027 |\n",
      "|    n_updates          | 328         |\n",
      "|    policyGradLoss     | -0.0454     |\n",
      "|    value_loss         | 0.347       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 610         |\n",
      "|    iterations         | 4           |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 688128      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.10394257  |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.846       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0226     |\n",
      "|    mean_step_reward   | 0.034067385 |\n",
      "|    n_updates          | 332         |\n",
      "|    policyGradLoss     | -0.0438     |\n",
      "|    value_loss         | 0.35        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 599         |\n",
      "|    iterations         | 5           |\n",
      "|    time_elapsed       | 68          |\n",
      "|    total_timesteps    | 696320      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.08392407  |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.91        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0693     |\n",
      "|    mean_step_reward   | 0.044833988 |\n",
      "|    n_updates          | 336         |\n",
      "|    policyGradLoss     | -0.036      |\n",
      "|    value_loss         | 0.269       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 599         |\n",
      "|    iterations         | 6           |\n",
      "|    time_elapsed       | 81          |\n",
      "|    total_timesteps    | 704512      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.09355737  |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0.874       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0268     |\n",
      "|    mean_step_reward   | 0.049272466 |\n",
      "|    n_updates          | 340         |\n",
      "|    policyGradLoss     | -0.0422     |\n",
      "|    value_loss         | 0.386       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 595         |\n",
      "|    iterations         | 7           |\n",
      "|    time_elapsed       | 96          |\n",
      "|    total_timesteps    | 712704      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.092489555 |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0.819       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0279      |\n",
      "|    mean_step_reward   | 0.04103516  |\n",
      "|    n_updates          | 344         |\n",
      "|    policyGradLoss     | -0.035      |\n",
      "|    value_loss         | 0.524       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 8           |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 720896      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.11773327  |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | 0.746       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0334     |\n",
      "|    mean_step_reward   | 0.036215823 |\n",
      "|    n_updates          | 348         |\n",
      "|    policyGradLoss     | -0.0485     |\n",
      "|    value_loss         | 0.504       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 592         |\n",
      "|    iterations         | 9           |\n",
      "|    time_elapsed       | 124         |\n",
      "|    total_timesteps    | 729088      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.093645714 |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | 0.802       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0253      |\n",
      "|    mean_step_reward   | 0.04148438  |\n",
      "|    n_updates          | 352         |\n",
      "|    policyGradLoss     | -0.0317     |\n",
      "|    value_loss         | 0.462       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 590         |\n",
      "|    iterations         | 10          |\n",
      "|    time_elapsed       | 138         |\n",
      "|    total_timesteps    | 737280      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.10057812  |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 0.841       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0282     |\n",
      "|    mean_step_reward   | 0.032800294 |\n",
      "|    n_updates          | 356         |\n",
      "|    policyGradLoss     | -0.0503     |\n",
      "|    value_loss         | 0.435       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 588         |\n",
      "|    iterations         | 11          |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 745472      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.112664275 |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0.847       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0156      |\n",
      "|    mean_step_reward   | 0.026330568 |\n",
      "|    n_updates          | 360         |\n",
      "|    policyGradLoss     | -0.0436     |\n",
      "|    value_loss         | 0.384       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 12          |\n",
      "|    time_elapsed       | 166         |\n",
      "|    total_timesteps    | 753664      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.09383686  |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 0.896       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0178     |\n",
      "|    mean_step_reward   | 0.031381838 |\n",
      "|    n_updates          | 364         |\n",
      "|    policyGradLoss     | -0.0424     |\n",
      "|    value_loss         | 0.315       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 13          |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 761856      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.098115854 |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0.874       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0557     |\n",
      "|    mean_step_reward   | 0.030791016 |\n",
      "|    n_updates          | 368         |\n",
      "|    policyGradLoss     | -0.0412     |\n",
      "|    value_loss         | 0.342       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 14          |\n",
      "|    time_elapsed       | 192         |\n",
      "|    total_timesteps    | 770048      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.0930569   |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0.851       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0159      |\n",
      "|    mean_step_reward   | 0.042292483 |\n",
      "|    n_updates          | 372         |\n",
      "|    policyGradLoss     | -0.046      |\n",
      "|    value_loss         | 0.372       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 599         |\n",
      "|    iterations         | 15          |\n",
      "|    time_elapsed       | 205         |\n",
      "|    total_timesteps    | 778240      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.1038578   |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0.846       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.019      |\n",
      "|    mean_step_reward   | 0.029724123 |\n",
      "|    n_updates          | 376         |\n",
      "|    policyGradLoss     | -0.0405     |\n",
      "|    value_loss         | 0.343       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 601         |\n",
      "|    iterations         | 16          |\n",
      "|    time_elapsed       | 217         |\n",
      "|    total_timesteps    | 786432      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.110143386 |\n",
      "|    entropy_loss       | -2.03       |\n",
      "|    explained_variance | 0.84        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0548     |\n",
      "|    mean_step_reward   | 0.032153323 |\n",
      "|    n_updates          | 380         |\n",
      "|    policyGradLoss     | -0.052      |\n",
      "|    value_loss         | 0.331       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 603         |\n",
      "|    iterations         | 17          |\n",
      "|    time_elapsed       | 230         |\n",
      "|    total_timesteps    | 794624      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.09289262  |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 0.86        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | -0.0442     |\n",
      "|    mean_step_reward   | 0.037766114 |\n",
      "|    n_updates          | 384         |\n",
      "|    policyGradLoss     | -0.052      |\n",
      "|    value_loss         | 0.344       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 601         |\n",
      "|    iterations         | 18          |\n",
      "|    time_elapsed       | 245         |\n",
      "|    total_timesteps    | 802816      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.089541286 |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0.893       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0367      |\n",
      "|    mean_step_reward   | 0.033554688 |\n",
      "|    n_updates          | 388         |\n",
      "|    policyGradLoss     | -0.0376     |\n",
      "|    value_loss         | 0.315       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 600         |\n",
      "|    iterations         | 19          |\n",
      "|    time_elapsed       | 259         |\n",
      "|    total_timesteps    | 811008      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.109220415 |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0.86        |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.127       |\n",
      "|    mean_step_reward   | 0.049187016 |\n",
      "|    n_updates          | 392         |\n",
      "|    policyGradLoss     | -0.0393     |\n",
      "|    value_loss         | 0.497       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 599        |\n",
      "|    iterations         | 20         |\n",
      "|    time_elapsed       | 273        |\n",
      "|    total_timesteps    | 819200     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.08068734 |\n",
      "|    entropy_loss       | -2.01      |\n",
      "|    explained_variance | 0.891      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.00584    |\n",
      "|    mean_step_reward   | 0.03537842 |\n",
      "|    n_updates          | 396        |\n",
      "|    policyGradLoss     | -0.0373    |\n",
      "|    value_loss         | 0.343      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 598        |\n",
      "|    iterations         | 21         |\n",
      "|    time_elapsed       | 287        |\n",
      "|    total_timesteps    | 827392     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.07031131 |\n",
      "|    entropy_loss       | -1.96      |\n",
      "|    explained_variance | 0.863      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0887     |\n",
      "|    mean_step_reward   | 0.05033692 |\n",
      "|    n_updates          | 400        |\n",
      "|    policyGradLoss     | -0.0351    |\n",
      "|    value_loss         | 0.625      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 596         |\n",
      "|    iterations         | 22          |\n",
      "|    time_elapsed       | 301         |\n",
      "|    total_timesteps    | 835584      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.07941884  |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 0.899       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.000561    |\n",
      "|    mean_step_reward   | 0.038771976 |\n",
      "|    n_updates          | 404         |\n",
      "|    policyGradLoss     | -0.0377     |\n",
      "|    value_loss         | 0.426       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 596        |\n",
      "|    iterations         | 23         |\n",
      "|    time_elapsed       | 316        |\n",
      "|    total_timesteps    | 843776     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.07966365 |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 0.835      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.156      |\n",
      "|    mean_step_reward   | 0.06280274 |\n",
      "|    n_updates          | 408        |\n",
      "|    policyGradLoss     | -0.0284    |\n",
      "|    value_loss         | 0.885      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 595         |\n",
      "|    iterations         | 24          |\n",
      "|    time_elapsed       | 330         |\n",
      "|    total_timesteps    | 851968      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06636518  |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | 0.867       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0974      |\n",
      "|    mean_step_reward   | 0.051047366 |\n",
      "|    n_updates          | 412         |\n",
      "|    policyGradLoss     | -0.0405     |\n",
      "|    value_loss         | 0.537       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 594         |\n",
      "|    iterations         | 25          |\n",
      "|    time_elapsed       | 344         |\n",
      "|    total_timesteps    | 860160      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.07532016  |\n",
      "|    entropy_loss       | -1.93       |\n",
      "|    explained_variance | 0.839       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.146       |\n",
      "|    mean_step_reward   | 0.049904786 |\n",
      "|    n_updates          | 416         |\n",
      "|    policyGradLoss     | -0.0409     |\n",
      "|    value_loss         | 0.684       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 594        |\n",
      "|    iterations         | 26         |\n",
      "|    time_elapsed       | 358        |\n",
      "|    total_timesteps    | 868352     |\n",
      "| train/                |            |\n",
      "|    approx_kl          | 0.08147277 |\n",
      "|    entropy_loss       | -1.92      |\n",
      "|    explained_variance | 0.883      |\n",
      "|    learning_rate      | 0.00025    |\n",
      "|    loss               | 0.0259     |\n",
      "|    mean_step_reward   | 0.05536133 |\n",
      "|    n_updates          | 420        |\n",
      "|    policyGradLoss     | -0.0421    |\n",
      "|    value_loss         | 0.507      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 593         |\n",
      "|    iterations         | 27          |\n",
      "|    time_elapsed       | 372         |\n",
      "|    total_timesteps    | 876544      |\n",
      "| train/                |             |\n",
      "|    approx_kl          | 0.06854673  |\n",
      "|    entropy_loss       | -1.9        |\n",
      "|    explained_variance | 0.872       |\n",
      "|    learning_rate      | 0.00025     |\n",
      "|    loss               | 0.0761      |\n",
      "|    mean_step_reward   | 0.057590336 |\n",
      "|    n_updates          | 424         |\n",
      "|    policyGradLoss     | -0.0332     |\n",
      "|    value_loss         | 0.571       |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_mean = -1e18\n",
    "trained = 0\n",
    "round_idx = 0\n",
    "\n",
    "try:\n",
    "    while trained < TOTAL_STEPS:\n",
    "        round_idx += 1\n",
    "        chunk = min(TRAIN_CHUNK, TOTAL_STEPS - trained)\n",
    "        # chunk = 2000\n",
    "\n",
    "        print(f\"\\n=== Round {round_idx} | Learn {chunk} steps (Total trained: {trained}) ===\")\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.learn(total_timesteps=chunk, reset_num_timesteps=False, tb_log_name='SF84G_1220')\n",
    "        trained += chunk\n",
    "\n",
    "        # --- Save Checkpoint ---\n",
    "        ckpt_path = os.path.join(CKPT_DIR, f\"SF84G_{int(trained/TRAIN_CHUNK)}.zip\")\n",
    "        model.save(ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        # --- Evaluate ---\n",
    "        mean_ret, best_ret = evaluate_policy(\n",
    "            model,\n",
    "            GAME,\n",
    "            STATE,\n",
    "            n_episodes=EVAL_EPISODES,\n",
    "            max_steps=EVAL_MAX_STEPS,\n",
    "        )\n",
    "        print(f\"[EVAL] Mean Return: {mean_ret:.3f}, Best Return: {best_ret:.3f}\")\n",
    "\n",
    "        # --- Save Best Model ---\n",
    "        if mean_ret > best_mean:\n",
    "            best_mean = mean_ret\n",
    "            best_path = os.path.join(LOG_DIR, \"best_model.zip\")\n",
    "            model.save(best_path)\n",
    "            print(f\"New best record. Saved to {best_path}\")\n",
    "\n",
    "        # --- Record Video ---\n",
    "        record_video(\n",
    "            model,\n",
    "            GAME,\n",
    "            STATE,\n",
    "            VIDEO_DIR,\n",
    "            video_len=RECORD_STEPS,\n",
    "            prefix=f\"step_{trained}_mean_{mean_ret:.2f}\",\n",
    "        )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted manually.\")\n",
    "\n",
    "finally:\n",
    "    train_env.close()\n",
    "    print(\"Training finished. Environment closed.\")\n",
    "    \n",
    "\"\"\"\n",
    "tensorboard --logdir=./runs_smw/tb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088b0b3-2418-4866-b332-0312c9f6467f",
   "metadata": {},
   "source": [
    "## Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73191bb-e875-4939-b04c-a4670abd9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Video\n",
    "# import glob\n",
    "\n",
    "# list_of_files = glob.glob(os.path.join(VIDEO_DIR, '*.mp4')) \n",
    "# if list_of_files:\n",
    "#     latest_file = max(list_of_files, key=os.path.getctime)\n",
    "#     print(f\"Playing: {latest_file}\")\n",
    "#     display(Video(latest_file, embed=True, width=600))\n",
    "# else:\n",
    "#     print(\"No videos found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab8 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
